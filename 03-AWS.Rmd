# AWS



## AWS S3

The AWS Command Line Interface (or AWS CLI) is an open source tool that enables an AWS user to interact with the AWS services from the command-line Shell. Below are described the most commonly used interactions with AWS using AWS CLI.

### Configure the AWS CLI service

1. To configure the service, please type "aws configure" on the console. This command is interactive, so the service will prompt you four times to enter some config information. Below an example (all RytenLab members have to ask me to send them their secret AWS credentials): 

```sh
$ aws configure 

AWS Access Key ID [None]: your_access_key 
AWS Secret Access Key [None]: your_secret_key 
Default region name [None]: eu-west-2 
Default output format [None]: json 
```

2. To check whether the connection with AWS has been correctly stablished, we can type the following command to list all our current buckets. 

```sh
$ aws s3 ls 
```

### Log out of AWS CLI

```sh
$ rm ~/.aws/config
$ rm ~/.aws/credentials
```

### Upload a single file to AWS using AWS CLI

Let's suppose we have a bucket on AWS called 'my-bucket'. Let's also suppose you have a file called 'myfile.txt' stored in your local that you would like to upload to AWS. To upload the file 'myfile.txt' to the bucket 'my-bucket':

```sh
$ aws s3 cp myfile.txt s3://my-bucket/ 
```

### Upload multiple files to AWS using AWS CLI

To upload multiple files, we can use the sync command. The **sync** command syncs objects under a specified local folder to files in a AWS bucket by uploading them to AWS.

```sh
$ aws s3 sync my_local_folder/ s3://my-bucket/ 
```

### Download a single file from AWS using AWS CLI

To download the file 'myfile.txt' from the 's3://my-bucket/' AWS bucket into your local folder:

```sh
$ aws s3 cp s3://my-bucket/myfile.txt ./my_local_folder 
```

### Download multiple files from AWS using AWS CLI

To download multiple files from an AWS bucket to your local folder, we can also use the **sync** command by changing the order of the parameters.

***Please, be aware the costs associated with downloading files correspond to $0.090 per GB - first 10 TB / month data transfer out beyond the global free tier.***

```sh
$ aws s3 sync s3://my-bucket/my_remote_folder/ ./my_local_folder 
```

### Requester payer 

Downloading/transferring data from AWS have associated some standard costs per GB transferred (~$0.09 per GB downloaded). To indicate AWS that those costs should be charged to the person (i.e. AWS account) performing the data downloading, it will be necessary to add the flag *--request-payer requester* to the *sync* command.

```{console}
## To list the objects stored within the bucket:
$ aws s3 ls s3://bucket_name/ --request-payer requester

## To download one single file from the bucket to your local folder:
$ aws s3 cp s3://bucket_name/myfile.txt ./your_local_folder  --request-payer requester

## To download all files from the bucket:
$ aws s3 sync s3://bucket_name/* ./your_local_folder  --request-payer requester

```

In addition, the S3 bucket containing the data needs to be configured as "Requester pays" (S3 -> Bucket -> Properties -> Requester pays -> Enable).

Finally, the bucket policy needs to be updated to grant permissions to the AWS account that is going to request the data:

```{json}
{
    "Version": "2012-10-17",
    "Id": "id_of_the_policy",
    "Statement": [
        {
            "Sid": "sid_of_the_policy",
            "Effect": "Allow",
            "Principal": {
                "AWS": [
                    "arn:aws:iam::iamcode:root",
                    "arn:aws:iam::iamcode:root"
                ]
            },
            "Action": [
                "s3:ListBucket",
                "s3:GetObject"
            ],
            "Resource": [
                "arn:aws:s3:::bucket_name",
                "arn:aws:s3:::bucket_name/*"
            ]
        }
    ]
}
```



### Checking AWS file integrity

Considering the example used in the previous section, let's check the integrity of the local folder './my_local_folder' matches with the integrity of the remote AWS folder 's3://my-bucket/my_local_folder/'.

1. First, clone the 'aws-s3-integrity-check' repo.

```console
$ git clone https://github.com/SoniaRuiz/aws-s3-integrity-check.git
```
2. Clone the 's3md5' repo.

```console
$ git clone https://github.com/antespi/s3md5.git
```
3. Move the s3md5 folder within the aws-s3-integrity-check folder:

```console
$ mv ./s3md5 ./aws-s3-integrity-check
```

4. Next, grant execution access permission to he s3md5 script file.

```console
$ chmod 755 ./aws-s3-integrity-check/s3md5/s3md5
```

5. The aws-s3-integrity-check folder should look similar to the following:

```console
total 16
-rw-r--r-- 1 your_user your_group 3573 date README.md
-rwxr-xr-x 1 your_user your_group 3301 date aws_check_integrity.sh
drwxr-xr-x 2 your_user your_group 4096 date s3md5
```

6. Execute the script 'aws_check_integrity.sh' following the this structure: **aws_check_integrity.sh 'local_path' 'bucket_name' 'bucket_folder'**

```console
$ aws_check_integrity.sh ./my_local_folder my-bucket my_local_folder/
```

### Creating a new AWS bucket

To create a new AWS bucket, we recommend using the following configuration:


1. Region: EU London
2. Block all public access: enabled 
3. Bucket Versioning: enable
4. Tags: 
    1. Key = "data-owner" / Value = "your name"
    2. Key = "data-origin" / Value = "the origin of the data in one word (i.e. the name of a project, the name of a server)"
5. Default encryption: enable - Amazon S3 key (SSE-S3)
6. Advanced settings
    1. Object Lock: enable/disable, depending on your type of data (more info [here](https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lock.html))
 





## AWS EC2

All commands here shown have been used to configure an EC2 virtual instance under the AWS free-tier.

### Generating an EC2 instance

To generate an EC2 instance under the free-tier using an Ubuntu 20.04 LTS image, please follow the next steps:

* Log in on the [AWS Console](https://eu-west-2.console.aws.amazon.com/console/home?region=eu-west-2) using your AWS account.
* Click on Services --> EC2 --> Instances.
* Click on the "Launch Instances" button. It is an orange button located on the upper right-hand side of the web page.

    * Step 1: Choose an Amazon Machine Image (AMI). Click on the "Free tier only" checkbox (located on the left-hand side menu) and select an AIM from the list. In this example I am going to use the "Ubuntu Server 20.04 LTS (HVM), SSD Volume Type" AMI.
    * Step 2: Choose an Instance Type. Select "t2.micro" (free tier eligible).
    * Step 3: Configure Instance Details. Leave all options by default and click "Next".
    * Step 4: Add Storage. Change the size of your disk from 8GB to 25GB. Free tier allows to get up to 30 GB of (SSD) disk storage.
    * Step 5: Add Tags. Add any tags or just leave it by default and click Next.
    * Step 6: Configure Security Group. We will need to add an additional HTTP rule and two custom TCP rules to set the ports to 3838 (for RServer to run) and 8787 (for RStudio to run). See "Step 5: Configuring the Security Group" section from this [post](https://jagg19.github.io/2019/08/aws-r/) for more details.
    * Step 7: Review and Launch. Review the info and click "Launch".
    * Step 8: Select or Create Key Pair. From the dropdown, choose "Create a new key pair". Name the .pem file and store it securily in your local computer. You will need later to connect to your EC2 instance.
    

### Connecting to an EC2 instance

SSH login to your EC2 instance:

* From your AWS Console. Click on "Services --> EC2 --> Instances". Check your newly created instance is running.
* Click on the "Connect" button and select the "SSH Client" tab. 
* Follow the instructions indicated there to connect to your EC2 instance from an SSH client (I normally use MobaXterm).


### Installing/Running RStudio on an EC2

```{bash, eval=FALSE}
## More info: https://jagg19.github.io/2019/08/aws-r/

## Update the Ubuntu repos 
# update indices
sudo apt update -qq
# install two helper packages we need
sudo apt install --no-install-recommends software-properties-common dirmngr
# add the signing key (by Michael Rutter) for these repos
# To verify key, run gpg --show-keys /etc/apt/trusted.gpg.d/cran_ubuntu_key.asc 
# Fingerprint: 298A3A825C0D65DFD57CBB651716619E084DAB9
sudo wget -qO- https://cloud.r-project.org/bin/linux/ubuntu/marutter_pubkey.asc | sudo tee -a /etc/apt/trusted.gpg.d/cran_ubuntu_key.asc
# add the R 4.0 repo from CRAN -- adjust 'focal' to 'groovy' or 'bionic' as needed
sudo add-apt-repository "deb https://cloud.r-project.org/bin/linux/ubuntu $(lsb_release -cs)-cran40/"


# Update ubuntu package repo, to get latest R
sudo apt update

# Install R
sudo apt -y install r-base r-base-dev

# Install shiny before shiny-server
sudo R -e "install.packages('shiny')"


# Install debian package manager, gdebi
sudo apt install gdebi-core


# Install Shiny Server (https://www.rstudio.com/products/shiny/download-server/)
sudo wget https://download3.rstudio.org/ubuntu-14.04/x86_64/shiny-server-1.5.17.973-amd64.deb
sudo gdebi shiny-server-1.5.17.973-amd64.deb
sudo rm shiny-server-1.5.17.973-amd64.deb


# Dependencies for R packages like RMariaDB, devtools, tidyverse, sparklyr. Please run seperate.
sudo apt -y install libcurl4-openssl-dev 
sudo apt -y install libssl-dev libxml2-dev libmariadbclient-dev build-essential libcurl4-gnutls-dev


# Install RStudio (https://www.rstudio.com/products/rstudio/download-server/)
wget https://download2.rstudio.org/server/bionic/amd64/rstudio-server-2021.09.2-382-amd64.deb
sudo gdebi rstudio-server-2021.09.2-382-amd64.deb
sudo rm rstudio-server-2021.09.2-382-amd64.deb


# Install some useful R Packages 
sudo R -e "install.packages('RCurl', repos='http://cran.rstudio.com')"
sudo R -e "install.packages('devtools', repos='http://cran.rstudio.com')"
sudo R -e "install.packages('tidyverse')"
sudo R -e "install.packages('RMariaDB')"


# Add user info to login RStudio
sudo adduser user_name
#Add rstudio to sudo group
sudo usermod -aG sudo user_name


# Install Java and reconfigure in R for RStudio use
sudo apt -y install default-jdk
sudo R CMD javareconf
# Change permissions for R library
sudo chmod 777 -R /usr/local/lib/R/site-library


#Restart rstudio-server to incorporate changes made in rserver.conf
sudo rstudio-server restart

```

### Access RStudio using your browser

RStudio should now be running on the newly generated AWS EC2 instance. To access RStudio from your local computer, please follow the next steps:

* From your AWS Console, click on "Services --> EC2 --> Instances".
* Click on the checkbox located next to the EC2 instance to select it.
* At the bottom of the web page it should have appeared a section with info from the selected instance.
* Copy the URL indicated under the "Public IPv4 DNS" section.
* Add the port 8787 to the URL (i.e. http://......:8787),
* Paste the URL on your browser. RStudio should now appear on your browser.

### Configuring HTTPS with RStudio using AWS EC2

Info extracted from [here](https://stackoverflow.com/questions/53102584/how-can-i-set-up-an-rstudio-server-to-run-with-ssl-on-aws) and [here](https://medium.com/@rachel_95942/configure-rstudio-server-with-ssl-on-aws-ec2-arm-e10fa2c36ef8).

**1. Create a new inbound rule to accept HTTPS traffic.**

* On the EC2 Management Console, navigate to 'Instances'.
* Select your instance of interest and click o the tab "Security" on the bottom panel.
* Click on the link located under the 'Security group' section.
* Click the button "Edit inbound rules".
* Add a two new rules with the following characteristics: 
    * Type: "HTTPs" - Source: "Custom" - value "::/0". 
    * Type: "HTTPs" - Source: "Custom" - value "0.0.0.0/0". 

**2. Configure the 'nginx' webserver to reverse proxy RStudio to use SSL**

* Install 'nginx' server:

```sh
sudo apt-get install nginx
``` 

* Create the SSL certificates: 

```sh
sudo mkdir /etc/nginx/ssl
sudo openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /etc/nginx/ssl/nginx.key -out /etc/nginx/ssl/nginx.crt
``` 

* Add the following configuration to the file 'sudo nano /etc/nginx/conf.d/rstudio.conf':

```sh
server {
  listen 80;
  listen [::]:80;

  listen 443 ssl;
  ssl_certificate /etc/nginx/ssl/nginx.crt;
  ssl_certificate_key /etc/nginx/ssl/nginx.key;

  server_name public_DNS_IP_of_your_AWS_instance;

  location / {
       proxy_pass http://localhost:8787/;
       proxy_redirect http://localhost:8787/ $scheme://$host/;
       proxy_http_version 1.1;
       proxy_set_header Upgrade $http_upgrade;
       proxy_set_header Connection $connection_upgrade;
       proxy_read_timeout 20d;
  }
}
```

* Edit the '/etc/nginx/nginx.conf' file and add the following lines to the 'http' block:
```sh
server_names_hash_bucket_size 128;

map $http_upgrade $connection_upgrade {
      default upgrade;
      ''      close;
          }
``` 

* Restart RStudio and nginx servers:
```sh
sudo rstudio-server restart
sudo systemctl restart nginx
``` 

**3.RStudio should be now running on port 443:**
```sh
https://public_DNS_IP_of_your_AWS_instance
``` 
### How to transfer files from EC2 to S3

[This](https://linuxhint.com/transfer-files-ec2-s3/) is a very good tutorial that explains how to transfer files from EC2 to S3 using AWS IAM Roles.

## AWS List of Cloud Services

```{r echo=F}
services <- c("Amazon API Gateway",
"Amazon Athena",
"Amazon Aurora",
"Amazon Aurora",
"Amazon Bracket",
"Amazon Carrier",
"Amazon Chime",
"Amazon CoudWatch",
"Amazon Code Guru Reviewer",
"Amazon Cognito",
"Amazon Comprehend",
"Amazon Comprehend Medical",
"Amazon DocumentDB",
"Amazon DynamoDB",
"Amazon EC2",
"Amazon EC2 Dedicated Hosts",
"Amazon EKS",
"Amazon Elastic Block Store (EBS)",
"Amazon Elastic Container Registry (ECR)",
"Amazon Elastic File System (EFS)",
"Amazon Elastic Graphics",
"Amazon Elastic IP	Static",
"Amazon Elastic Transcoder",
"Amazon ElastiCache",
"Amazon Elasticsearch Service",
"Amazon EMR",
"Amazon FSx for Lustre",
"Amazon FSx for Windows File Server",
"Amazon GuardDuty",
"Amazon Kinesis Data Analytics",
"Amazon Kinesis Data Firehose",
"Amazon Kinesis Data streams",
"Amazon Lookout for Vision",
"Amazon Managed Streaming for Apache Kafka (MSK)",
"Amazon MQ",
"Amazon Neptune",
"Amazon Polly",
"Amazon QuickSight",
"Amazon RDS for MariaDB",
"Amazon RDS for MySQL",
"Amazon RDS for Oracle",
"Amazon RDS for PostgreSQL",
"Amazon RDS for SQL server",	
"Amazon Redshift",
"Amazon Rekognition",
"Amazon Route 53",
"Amazon S3 Glacier",	
"Amazon SageMaker",
"Amazon SageMaker Ground Truth",
"Amazon Simple Email Service",
"Amazon Simple Notification Service",	
"Amazon Simple Queue Service (SQS)",	
"Amazon S3",
"Amazon Simple Workflow Service (SWF)")	


description <- c(
"Service to develop APIs. These APIs will act as the front door for applications to access the data from the backend services behind the API.",
"SQL to analyse data stored in S3.",
"MySQL-Compatible	Relational database for the cloud. ",
"PostgreSQL-Compatible	Relational database for the cloud.",
"Quantum computing service.",
"A Carrier IP address is the address that I will assign to a network interface (for example an EC2 instance).",
"To let users meet and chat online.",
"Monitoring and management service providing insights for AWS.",
"Reviewer	Service that uses program analysis and machine learning to detect potential defects that are difficult for developers to detect.",
"To add user sign-up, sign-in and access control to my web and mobile apps.",
"For text processing and analysis.",
"HIPAA-eligible natural language processing (NLP)",
"Document database service",
"Is a key-value and document database that delivers single-digit millisecond performance at any scale.",
"Compute platform",
"Allows you to use your eligible software licences on Amazon EC2.",
"Amazon Elastic Kubernetes Service",
"Allows you to create persistent block storage volumes and attach them to Amazon EC2 instances.",
"To store, manage, and deploy Docker container images.",
"Provides a file system for use with AWS Cloud services.",
"Allows you to attach low-cost graphics acceleration to EC2 instances.",
"IPv4 for dynamic cloud computing.",
"Is media transcoding in the cloud. Designed to convert (or “transcode”) media files from their source format into versions that will playback on devices like smartphones.",
"Build data-intensive apps or improve the performance of your existing apps by retrieving data from high throughput and low latency in-memory data stores.",
"To operate Elasticsearch at scale with zero downtime.",
"Industry-leading cloud big data platform for processing vast amounts of data using open-source tools. EMR makes easy to set up, operate, and scale your big-data environments by automating time-consuming tasks like provisioning capacity and tuning clusters. To run petabyte-scale analysis at less than half of the cost of traditional on-premises solutions.",
"Integrated with S3, is designed for fast processing of workloads.",
"To move your Windows based applications that require file storage to AWS.",
"Is a threat detection service that monitors for malicious activity to protect your AWS accounts and data stored in Amazon S3.",
"To analyse streaming data (for responding to your business and customer needs in real time)",
"To only pay for the volume of data ingested into the service.",
"Data streaming service.",
"Machine learning (ML) service that spots defects and anomalies in visual representations using computer vision (CV).",
"To easily run applications that use Apache Kafka",
"To set up and operate message brokers in the cloud.",
"Graph database service to build and run applications that work with highly connected datasets.",
"Service that turns text into lifelike speech.",
"Business intelligence service to deliver insights to everyone in your organization.",
"To set up, operate and scale MariaDB database deployments in the cloud.",
"''",
"",
"",
"",
"Petabyte-scale data warehouse",
"Images and video analysis recognition; to identify objects, people, text, scenes, etc. Also, to detect inappropriate content.",
"DNS web service",
"",
"To manage costs associated to ML instances",
"To build training datasets for machine learning.",
"Cloud-based email sending service.",
"Amazon Simple Notification Service	",
"Amazon Simple Queue Service (SQS)	",
"Amazon Simple Storage Service (S3).",
"Amazon Simple Workflow Service (SWF)"	
)


AWS_services <- data.frame(name = services,
                           description = description)

knitr::kable(
  AWS_services, booktabs = TRUE,
  caption = 'list of AWS cloud services.'
)

```




\documentclass[]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\PassOptionsToPackage{hyphens}{url} % url is loaded by hyperref
\usepackage[unicode=true]{hyperref}
\hypersetup{
            pdftitle={Knowledge Book},
            pdfauthor={Sonia García-Ruiz (s.ruiz@ucl.ac.uk)},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{natbib}
\bibliographystyle{apalike}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\usepackage{longtable,booktabs}
% Fix footnotes in tables (requires footnote package)
\IfFileExists{footnote.sty}{\usepackage{footnote}\makesavenoteenv{long table}}{}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage{booktabs}

\title{Knowledge Book}
\author{Sonia García-Ruiz
(\href{mailto:s.ruiz@ucl.ac.uk}{\nolinkurl{s.ruiz@ucl.ac.uk}})}
\date{2022-01-20}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\chapter{Analysis of Genetic Association
Studies}\label{analysis-of-genetic-association-studies}

\emph{All information shown in this chapter has been extracted from the
course ``Analysis of Genetic Association Studies'', taught at University
of Liverpool, Department of Health Data Science.}

The purpose of this section is to provide guidance on performing a GWAS
analysis step-by step, as well as introducing to useful software for
undertaking a GWAS.

\section{Useful Linux commands}\label{useful-linux-commands}

First, I'll show you some useful Linux commands for working with GWASs
files. Linux is an operating system very well suited to dealing with
large datasets.

Create a bash script:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{touch}\NormalTok{ script.sh}
\end{Highlighting}
\end{Shaded}

Run the script:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sh}\NormalTok{ script.sh}
\end{Highlighting}
\end{Shaded}

Return the number of lines of a file:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{wc}\NormalTok{ -l ./file.map}
\end{Highlighting}
\end{Shaded}

Print only the first 15 lines of ``./file.map'' file:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{ -15 ./file.map}
\end{Highlighting}
\end{Shaded}

Print the bottom of the file:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tail}\NormalTok{ -15 ./file.map}
\end{Highlighting}
\end{Shaded}

Find the SNP ``rs1234'':

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{grep} \StringTok{"rs1234"}\NormalTok{ ./file.map}
\end{Highlighting}
\end{Shaded}

Find the SNP ``rs1234'' and print the line number of the match:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{grep}\NormalTok{ -n }\StringTok{"rs1234"}\NormalTok{ ./file.map}
\end{Highlighting}
\end{Shaded}

Search the file ./file.map, for ``1234'' in the SNP name (column 2,
specified by \$2) and print the whole line:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{awk} \StringTok{'$2~/1234/\{print\}'}\NormalTok{ ./file.map}
\end{Highlighting}
\end{Shaded}

Search the file ./file.map, for ``1234'' in the SNP position (column 4,
specified by \$4) and print only the SNP name (column 2):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{awk} \StringTok{'$4~/1234/\{print $2\}'}\NormalTok{ ./file.map}
\end{Highlighting}
\end{Shaded}

Search for an exact match of ``rs1234676'' in the SNP name (column 2)
and print the whole line:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{awk} \StringTok{'$2=="rs1234676"/\{print\}'}\NormalTok{ ./file.map}
\end{Highlighting}
\end{Shaded}

Search for all SNPs with a position between 90680000 up to and including
90690000 in the SNP position column (column 4) and count the lines in
that output:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{awk} \StringTok{'$4>90680000&&$4<=90690000\{print\}'}\NormalTok{ ./file.map }\KeywordTok{|} \FunctionTok{wc}\NormalTok{ -l}
\end{Highlighting}
\end{Shaded}

Remove the chromosome 10 from the output above:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{awk} \StringTok{'$4>90680000&&$4<=90690000\{print\}'}\NormalTok{ ./file.map }\KeywordTok{|} \FunctionTok{awk} \StringTok{'$1!=10\{print\}'}
\end{Highlighting}
\end{Shaded}

Count the SNPs from chromosome 10 and chromosome 23:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{awk} \StringTok{'$1==10\{print\}'}\NormalTok{ ./file.map}\KeywordTok{|} \FunctionTok{wc}\NormalTok{ -l}
\FunctionTok{awk} \StringTok{'$1==23\{print\}'}\NormalTok{ ./file.map}\KeywordTok{|} \FunctionTok{wc}\NormalTok{ -l}
\end{Highlighting}
\end{Shaded}

Print the SNP position of the first SNP on chromosome 23:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{awk} \StringTok{'$1==23\{print $4\}'}\NormalTok{ ./file.map }\KeywordTok{|} \FunctionTok{head}\NormalTok{ -1}
\end{Highlighting}
\end{Shaded}

There are SNPs starting with other characters than ``rs'', identify what
other starting characters there are.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{awk} \StringTok{'$2!~/rs/\{print $2\}'}\NormalTok{ ./file.map}
\FunctionTok{awk} \StringTok{'$2!~/rs/&&$2!~/.../\{print $2\}'}\NormalTok{ ./file.map}
\end{Highlighting}
\end{Shaded}

\section{Useful R commands}\label{useful-r-commands}

NOTE: R has a system of libraries on the Comprehensive R Archive Network
(CRAN) where R users deposit code to be verified and then distributed
for wider use (\url{https://cran.r-project.org/}).

Load the file ``./file.map'' into R.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gd <-}\StringTok{ }\KeywordTok{read.table}\NormalTok{(}\StringTok{"./file.map"}\NormalTok{,}\DataTypeTok{header=}\NormalTok{F)}
\KeywordTok{names}\NormalTok{(gd)<-}\KeywordTok{c}\NormalTok{(}\StringTok{"CHR"}\NormalTok{,}\StringTok{"SNP"}\NormalTok{,}\StringTok{"cM"}\NormalTok{,}\StringTok{"BP"}\NormalTok{)}
\KeywordTok{head}\NormalTok{(gd)}
\KeywordTok{tail}\NormalTok{(gd)}
\end{Highlighting}
\end{Shaded}

Search for only rs1234568 in the SNP name (column 2):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gd[gd}\OperatorTok{$}\NormalTok{SNP}\OperatorTok{==}\StringTok{"rs1234568"}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

Search for all SNPs with a position between 90680000 up to and including
90690000 in the SNP position column:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gd[gd}\OperatorTok{$}\NormalTok{BP}\OperatorTok{>}\DecValTok{90680000}\OperatorTok{&}\NormalTok{gd}\OperatorTok{$}\NormalTok{BP}\OperatorTok{<=}\DecValTok{90690000}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

Do you have any genome-wide significant (p=5x10-8) SNPs?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gd[gd}\OperatorTok{$}\NormalTok{P}\OperatorTok{<}\FloatTok{0.00000005}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

Report results using a Manhattan plot:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(gd}\OperatorTok{$}\NormalTok{BP,}\OperatorTok{-}\KeywordTok{log}\NormalTok{(gd}\OperatorTok{$}\NormalTok{P,}\DataTypeTok{base=}\DecValTok{10}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

There were SNPs from both chromosome 10 and 23 in the .map file.
Separate chromosomes by colour:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(gd}\OperatorTok{$}\NormalTok{BP,}\OperatorTok{-}\KeywordTok{log}\NormalTok{(gd}\OperatorTok{$}\NormalTok{P,}\DataTypeTok{base=}\DecValTok{10}\NormalTok{), }\DataTypeTok{col=}\NormalTok{gd}\OperatorTok{$}\NormalTok{CHR)}
\end{Highlighting}
\end{Shaded}

Add a reference line to know which SNPs have a p-value below 1x10-5:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(gd}\OperatorTok{$}\NormalTok{BP,}\OperatorTok{-}\KeywordTok{log}\NormalTok{(gd}\OperatorTok{$}\NormalTok{P,}\DataTypeTok{base=}\DecValTok{10}\NormalTok{), }\DataTypeTok{col=}\NormalTok{gd}\OperatorTok{$}\NormalTok{CHR)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{h=}\OperatorTok{-}\KeywordTok{log}\NormalTok{(}\FloatTok{1e-5}\NormalTok{,}\DataTypeTok{base=}\DecValTok{10}\NormalTok{),}\DataTypeTok{col=}\StringTok{"blue"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Use the package ``qqman'' to generate a Manhattan plot:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{manhattan}\NormalTok{(gd)}
\KeywordTok{manhattan}\NormalTok{(gd[gd}\OperatorTok{$}\NormalTok{chromosome}\OperatorTok{==}\DecValTok{10}\NormalTok{,], }\DataTypeTok{chr=}\StringTok{"chromosome"}\NormalTok{,}\DataTypeTok{bp=}\StringTok{"position"}\NormalTok{,}\DataTypeTok{snp=}\StringTok{"rsid"}\NormalTok{,}\DataTypeTok{p=}\StringTok{"pvalue"}\NormalTok{)}
\KeywordTok{manhattan}\NormalTok{(gd[gd}\OperatorTok{$}\NormalTok{chromosome}\OperatorTok{==}\DecValTok{23}\NormalTok{,], }\DataTypeTok{chr=}\StringTok{"chromosome"}\NormalTok{,}\DataTypeTok{bp=}\StringTok{"position"}\NormalTok{,}\DataTypeTok{snp=}\StringTok{"rsid"}\NormalTok{,}\DataTypeTok{p=}\StringTok{"pvalue"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Variant Calling from sequencing
data}\label{variant-calling-from-sequencing-data}

\subsection{Generate genotype calls for a single
sample}\label{generate-genotype-calls-for-a-single-sample}

In this section we will generate genotype calls from aligned sequencing
data, explore the calls and perform some initial quality checks.

A commonly used variant caller comes from the Genome Analysis Toolkit
(GATK).

\begin{itemize}
\tightlist
\item
  GATK takes files containing aligned sequencing reads. These are called
  .bam files.
\item
  GATK produces .vcf files containing details of the variants, to
  explore and manipulate these files, we'll be using vcftools. This is
  also freely available and can be found:
  \url{http://vcftools.sourceforge.net/}
\item
  To visualise the variant calls we'll be using Integrated Genome Viewer
  (IGV); this is available from:
  \url{http://software.broadinstitute.org/software/igv/}
\end{itemize}

Usually, each .bam file has an accompanying .bai file. These are index
files, which are programs used to navigate the data faster -- they must
be present for software programs to run.

Generate genotype calls for the sample NA12878 using GATK:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{gatk}\NormalTok{ -T HaplotypeCaller \textbackslash{}}
\NormalTok{-R human_g1k_b37_20.fasta \textbackslash{}}
\NormalTok{-I NA12878_wgs_20.bam \textbackslash{}}
\NormalTok{-o yourfilename.vcf \textbackslash{}}
\NormalTok{-L 20:10,000,000-10,200,000}
\end{Highlighting}
\end{Shaded}

Variant calling on a whole genome can take a long time, we're only
looking at a very small region: Chromosome 20, positions 10,000,000 --
10,200,000, specified by the --L option.

(*) Details of the different options used:
\url{https://software.broadinstitute.org/gatk/documentation/tooldocs/current/org_broadinstitute_gatk_tools_walkers_haplotypecaller_HaplotypeCaller.php}

To have a look at the first 5 variant calls using:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{ -34 yourfilename.vcf }\KeywordTok{|} \FunctionTok{tail}\NormalTok{ -6}
\end{Highlighting}
\end{Shaded}

Filter the vcf file and see only the variants called in the region
20:10,002,371-10,002,546:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{vcftools}\NormalTok{ --vcf yourfilename.vcf \textbackslash{}}
\NormalTok{--chr 20 \textbackslash{}}
\NormalTok{--from-bp 10002371 \textbackslash{}}
\NormalTok{--to-bp 10002546 \textbackslash{}}
\NormalTok{--out yournewfilename \textbackslash{}}
\NormalTok{--recode}
\end{Highlighting}
\end{Shaded}

The code above will generate the files: yournewfilename.recode.vcf and
yournewfilename.log.

To know how many variants were identified in this region:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{ yournewfilename.log}
\end{Highlighting}
\end{Shaded}

What are those variants?

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{ yournewfilename.recode.vcf}
\end{Highlighting}
\end{Shaded}

\subsection{Generating genotypes from multiple .bam
files}\label{generating-genotypes-from-multiple-.bam-files}

To generate genotypes for multiple samples, one option is to run the
same command than above but give multiple -I options:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{gatk}\NormalTok{ -T HaplotypeCaller \textbackslash{}}
\NormalTok{-R human_g1k_b37_20.fasta \textbackslash{}}
\NormalTok{-I NA12878_wgs_20.bam \textbackslash{}}
\NormalTok{-I NA12882_wgs_20.bam \textbackslash{}}
\NormalTok{-I NA12877_wgs_20.bam \textbackslash{}}
\NormalTok{-o differentfilename.vcf \textbackslash{}}
\NormalTok{-L 20:10,000,000-10,200,000}
\end{Highlighting}
\end{Shaded}

Another option is to run each sample independently, but generate a
.g.vcf file and then merge the .g.vcf files to get a single call set.

Generate a .g.vcf file for sample NA12878 (then run the same command on
the bam files for samples NA12877 and NA12882):

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{gatk}\NormalTok{ -T HaplotypeCaller \textbackslash{}}
\NormalTok{-R human_g1k_b37_20.fasta \textbackslash{}}
\NormalTok{-I NA12878_wgs_20.bam \textbackslash{}}
\NormalTok{-o yourchoice_NA12878.g.vcf \textbackslash{}}
\NormalTok{-ERC GVCF \textbackslash{}}
\NormalTok{-L 20:10,000,000-10,200,000}
\end{Highlighting}
\end{Shaded}

The main difference between the .g.vcf and the .vcf file is that the
gvcf file contains a line per base or region if the region contains no
variants. The gvcf also includes as a symbolic allele, this allows for
complex variants at the site and to makes it possible to give a
confidence measure in the reference allele (more info:
\url{http://gatkforums.broadinstitute.org/gatk/discussion/4017/what-is-a-gvcf-and-how-is-it-differentfrom-a-regular-vcf})

Merge the individual .g.vcf files and get a .vcf file containing the
calls for all samples:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{gatk}\NormalTok{ -T GenotypeGVCFs \textbackslash{}}
\NormalTok{-R human_g1k_b37_20.fasta \textbackslash{}}
\NormalTok{-V yourchoice_NA12878.g.vcf \textbackslash{}}
\NormalTok{-V yourchoice_NA12882.g.vcf \textbackslash{}}
\NormalTok{-V yourchoice_NA12877.g.vcf \textbackslash{}}
\NormalTok{-o yourchoice_gvcf_jointcalls.vcf \textbackslash{}}
\NormalTok{-L 20:10,000,000-10,200,000}
\end{Highlighting}
\end{Shaded}

\subsection{Performing initial QC}\label{performing-initial-qc}

GWAS studies often focus only on SNPS, whereas variant callers identify
more variants, such as indels. To create a file containing just SNPs (no
indels or other variants) the --remove-indels option from vcftools can
be used:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{vcftools}\NormalTok{ --vcf yourchoice_gvcf_jointcalls.vcf \textbackslash{}}
\NormalTok{--remove-indels \textbackslash{}}
\NormalTok{--out yourchoice_snps \textbackslash{}}
\NormalTok{--recode}
\end{Highlighting}
\end{Shaded}

Calculate the Ts/Tv ratio:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{vcftools}\NormalTok{ --vcf yourchoice_gvcf_jointcalls.vcf \textbackslash{}}
\NormalTok{--TsTv-summary \textbackslash{}}
\NormalTok{--out yourchoice_tstv}
\end{Highlighting}
\end{Shaded}

The QualByDepth (QD) score is the variant quality (QUAL field) divided
by the unfiltered depth of nonreference samples. Variants with a low QD
may be unreliable and have a high false positive rate. To add a filter
to the .vcf file which flags variants which have a quality-by-depth less
than 2.0:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{gatk}\NormalTok{ -T VariantFiltration \textbackslash{}}
\NormalTok{-R human_g1k_b37_20.fasta \textbackslash{}}
\NormalTok{-V yourchoice_gvcf_jointcalls.vcf \textbackslash{}}
\NormalTok{--filterExpression }\StringTok{'QD<2.0'}\NormalTok{ \textbackslash{}}
\NormalTok{--filterName }\StringTok{'QualityDepth'}\NormalTok{ \textbackslash{}}
\NormalTok{-o yourchoice_flagged.vcf}
\end{Highlighting}
\end{Shaded}

NOTE: SNPs and Indels may need to be treated differently -- to do this,
the file can be first splitted into 2 vcfs, one containing SNPs and one
containing indels, then add the flags should be added.

The new .vcf is now different from the original as the FILTER column is
now filled in, usually with PASS.

Remove all SNPs with a filter flag other than PASS:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{vcftools}\NormalTok{ --vcf yourchoice_flagged.vcf \textbackslash{}}
\NormalTok{--remove-filtered-all \textbackslash{}}
\NormalTok{--recode \textbackslash{}}
\NormalTok{--out yourchoice_filtered}
\end{Highlighting}
\end{Shaded}

To convert the SNP filtered .vcf file to PLINK .ped and .map files:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{vcftools}\NormalTok{ --vcf yourchoice_gvcf_jointcalls.vcf \textbackslash{}}
\NormalTok{--remove-indels \textbackslash{}}
\NormalTok{--out yourchoice_plink}
\end{Highlighting}
\end{Shaded}

\section{Quality Control for GWAS}\label{quality-control-for-gwas}

\subsection{Software and datasets}\label{software-and-datasets}

A commonly used program for GWAS analysis is called `PLINK', which is
freely available for download. The webpage for PLINK is available at:
\url{http://pngu.mgh.harvard.edu/~purcell/plink/} For undertaking a
GWAS, two input files are required: one `.ped' file and one `.map' file.

\begin{itemize}
\tightlist
\item
  The PED file is a white-space (space or tab) delimited file. It
  includes one row for each participant within the study, and at least
  seven columns. Each of the SNPs genotyped is represented by two
  columns (column 7 onwards) --one for each of the two alleles held by
  the individual at the SNP. Typical coding would be A, C, G or T to
  represent the four possible alleles, or similarly 1, 2, 3 or 4. Coding
  for missing genotype is `0 0'.
\item
  The .map file includes information on the SNPs genotyped, and each row
  represents a SNP. It includes 4 columns.
\end{itemize}

In this example, we will be working on analysing the GWAS dataset
`genstudy' (a .map and .ped file for `genstudy'). The study is a
case-control study with 252 diseased (cases) and 252 non-diseased
(controls) individuals. All individuals have been genotyped for 198,684
SNPs on chromosome 10 and 22,379 SNPs on chromosome X. The purpose of
the study is to try and identify SNPs on chromosome 10 associated with
disease.

To view the first row and first eight columns of the `genstudy.ped'
file:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{awk} \StringTok{'FNR == 1 \{ print $1,$2,$3,$4,$5,$6,$7,$8\} '}\NormalTok{ genstudy.ped}
\end{Highlighting}
\end{Shaded}

The eight columns correspond to the first six mandatory columns plus two
columns representing the first SNP genotyped.

\subsection{Checking the input
datasets}\label{checking-the-input-datasets}

Before starting a genotype QC or GWAS association analysis, it is
important to check that the input files are in good order, and contain
the individuals and SNPs that we think they should. Do this by focussing
on the SNPs on chromosome 10:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{plink}\NormalTok{ --file genstudy --chr 10 --noweb}
\end{Highlighting}
\end{Shaded}

The output will also be saved in a log file called `plink.log'.

Note: the `-noweb' option is included to make PLINK run faster --
otherwise it connects to the web to check for updates before running the
analysis.

\subsection{Binary PED files}\label{binary-ped-files}

To save space and time, .map and .ped files can be converted to binary
format. In doing this, we create three new files as follows:

\begin{itemize}
\tightlist
\item
  `.bed': file This includes all the genotype data from the previous
  `.ped' file, but in binary format.
\item
  `.fam': file This includes the first six columns from the previous
  `.ped' file.
\item
  `.bim' file This is the same as the previous `.map' file, but also
  includes an additional two columns with the alleles names (which are
  otherwise lost when'converting the genotype data from .ped file to
  .bed file.)
\end{itemize}

To create a set of binary format files for the `genstudy' dataset:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{plink}\NormalTok{ --file genstudy --make-bed --out genstudy --noweb}
\end{Highlighting}
\end{Shaded}

To use the binary format files instead of the .ped and .map files, we
just substitute the option --file with --bfile in the PLINK command
line:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{plink}\NormalTok{ --bfile genstudy --chr 10 --noweb}
\end{Highlighting}
\end{Shaded}

\subsection{Sample QC}\label{sample-qc}

In this section, we will start undertaking genotype QC on the `genstudy'
dataset -- starting first of all with sample QC, one step at a time.

\subsubsection{Checks for missing data}\label{checks-for-missing-data}

To obtain a summary of the amount of missing genotype data per sample
and per SNP, use the option `- -missing' in the PLINK command line.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{plink}\NormalTok{ --bfile genstudy --missing --out genstudy.CRstats --noweb}
\end{Highlighting}
\end{Shaded}

Adding the `--out genstudy.CRstats' option ensures that the two output
files are called `genstudy.CRstats.imiss' and `genstudy.CRstats.lmiss'
respectively.

This command creates two output files:

\begin{itemize}
\tightlist
\item
  a `.imiss' file: summarises the proportion of missing genotype data
  per individual (one row per individual and six columns). The most
  important column for sample QC purposes is column 6, which contains
  the proportion of missing genotypes for the individual.
\item
  a `.lmiss' file: summarises the proportion of missing genotype data
  per SNP (one row per SNP and five columns). The most important column
  for SNP QC purposes is column 5, which contains the proportion of
  missing genotypes for the SNP.
\end{itemize}

\subsubsection{Gender Checks}\label{gender-checks}

PLINK allows to use genotype data from the X chromosome to determine
gender (i.e.~based on heterozygosity rates), and compares this to
reported gender as per the .ped/.fam file. PLINK then flags any
discrepancies i.e.~individuals for whom reported gender does not match
the gender estimated based on genotype data. To undertake this
comparison, we can use the option `- - check-sex' in PLINK.

To undertake the gender comparison and save results in an output file
called `genstudy.sexcheck':

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{plink}\NormalTok{ --bfile genstudy --check-sex --out genstudy --noweb}
\end{Highlighting}
\end{Shaded}

The output file has six columns, being the most important one (for the
purpose of sample QC) column 5, which indicates whether there is a
gender discordance or not.

\subsubsection{Duplicate/related
samples}\label{duplicaterelated-samples}

PLINK will also calculated identity-by-state (IBS) and
identity-by-descent (IBD) statistics for our individuals to help us
identify duplicated and/or related samples. Before we ask PLINK to do
this, however, we need to generate a pruned subset of SNPs using the
option `--indep-pairwise' in PLINK, which is based on pairwise genotypic
correlation. An example of a command line for generating a pruned subset
of SNPs is as follows:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{plink}\NormalTok{ --bfile genstudy --indep-pairwise 1500 150 0.2 --noweb}
\end{Highlighting}
\end{Shaded}

The options 1500, 150 and 0.2 in the command above relate to the
following steps, and of course these options can be changed depending on
your requirements:

\begin{itemize}
\tightlist
\item
  consider a window of 1500 SNPs,
\item
  calculate LD between each pair of SNPs in the window, remove one of a
  pair of SNPs if the LD is greater than 0.2,
\item
  shift the window 150 SNPs forward and repeat the procedure
\end{itemize}

Running this command line will create two output files, `plink.prune.in'
and `plink.prune.out':

\begin{itemize}
\tightlist
\item
  plink.prune.in: lists all SNPs remaining after pruning
\item
  plink.prune.out: lists all SNPs removed during pruning
\end{itemize}

Both these files can subsequently be specified as the argument for
PLINK's `--extract' or `--exclude' command respectively to provide a
pruned set of SNPs.

To create a pruned version of the `genstudy' dataset and call it
`p.genstudy':

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{plink}\NormalTok{ --bfile genstudy --extract plink.prune.in --make-bed --out p.genstudy --noweb}
\end{Highlighting}
\end{Shaded}

To generate IBS and IBD estimates for all pairs of individuals, use the
PLINK option `---genome':

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{plink}\NormalTok{ --bfile p.genstudy --genome --out genstudy --noweb}
\end{Highlighting}
\end{Shaded}

Running this command creates an output file called ``genstudy.genome''.
This includes a row per each unique pair of individuals, and fourteen
columns, being the most important for the purpose of sample QC column
10, as it includes an estimate of the IBD proportion between
individuals.

\subsubsection{Heterozygosity}\label{heterozygosity}

To run heterozygosity checks in PLINK, we can use the option `- -het':

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{plink}\NormalTok{ --bfile genstudy --het --out genstudy}
\end{Highlighting}
\end{Shaded}

This command produces the output file ``genstudy.het'', which contains
one row per individual and six columns. Column 6 includes the estimate
of the inbreeding coefficient, F.

\subsubsection{Removing individuals who fail sample
QC}\label{removing-individuals-who-fail-sample-qc}

To remove individuals who fail any of the QC steps above.

\paragraph{Removing due to
missingness}\label{removing-due-to-missingness}

To investigate how many individuals are excluded at difference threshold
levels (e.g.~1\%, 5\%, and 10\%):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{awk} \StringTok{'NR>1 && $6 >= 0.01 \{print\}'}\NormalTok{ genstudy.CRstats.imiss }\KeywordTok{|} \FunctionTok{wc}\NormalTok{ -l}
\FunctionTok{awk} \StringTok{'NR>1 && $6 >= 0.05 \{print\}'}\NormalTok{ genstudy.CRstats.imiss }\KeywordTok{|} \FunctionTok{wc}\NormalTok{ -l}
\FunctionTok{awk} \StringTok{'NR>1 && $6 >= 0.1 \{print\}'}\NormalTok{ genstudy.CRstats.imiss }\KeywordTok{|} \FunctionTok{wc}\NormalTok{ -l}
\end{Highlighting}
\end{Shaded}

To exclude all individuals with more than 5\% missing data:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{awk} \StringTok{'$6 >= 0.05 \{print\}'}\NormalTok{ genstudy.CRstats.imiss }\OperatorTok{>}\NormalTok{ genstudy.CRremove}
\ExtensionTok{plink}\NormalTok{ --bfile genstudy --noweb --remove genstudy.CRremove --make-bed --out genstudy.CR}
\end{Highlighting}
\end{Shaded}

\paragraph{Removing due to gender
discordance}\label{removing-due-to-gender-discordance}

To select only those individuals with an entry of `PROBLEM' in column 5:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{awk} \StringTok{'$5=="PROBLEM"\{print\}'}\NormalTok{ genstudy.sexcheck}\OperatorTok{>}\NormalTok{genstudy.sexremove}
\end{Highlighting}
\end{Shaded}

To remove these individuals from the `genstudy.CR' dataset, and create a
new dataset named `genstudy.sex':

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{plink}\NormalTok{ --bfile genstudy.CR --noweb --remove genstudy.sexremove --make-bed --out genstudy.sex}
\end{Highlighting}
\end{Shaded}

\paragraph{Remove duplicates and related
samples}\label{remove-duplicates-and-related-samples}

We use an IBD threshold of 0.1875 to filter out either duplicated or
closely related samples (column 10).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{awk} \StringTok{'$10 >= 0.1875 \{print\}'}\NormalTok{ genstudy.genome }\OperatorTok{>}\NormalTok{ genstudy.PIremove}
\end{Highlighting}
\end{Shaded}

To remove this list of individuals from the `genstudy.sex' dataset and
create a new dataset `genstudy\_sampleqc':

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{plink}\NormalTok{ --bfile genstudy.sex --noweb --remove genstudy.PIremove.list --make-bed --out genstudy_sampleqc}
\end{Highlighting}
\end{Shaded}

\paragraph{Heterozygosity}\label{heterozygosity-1}

Look at the ``genstudy.Het\_vs\_imiss.pdf'' and see if outliers exist.
If not, there are no samples to remove.

\subsubsection{SNP QC}\label{snp-qc}

\paragraph{Checks for missing data}\label{checks-for-missing-data-1}

Similar to what we did for sample QC, we can investigate how many SNPs
fail different missingness thresholds by filtering column 5 in
`genstudy\_sampleqc.CRstats.lmiss' using different threshold values. We
can do this using the following command (e.g.~for threshold of 1\%):

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{plink}\NormalTok{ --bfile genstudy_sampleqc --missing --out genstudy_sampleqc.CRstats --noweb}
\FunctionTok{awk} \StringTok{'NR>1 && $5 >= 0.01 \{print\}'}\NormalTok{ genstudy_sampleqc.CRstats.lmiss }\KeywordTok{|} \FunctionTok{wc}\NormalTok{ -l}
\end{Highlighting}
\end{Shaded}

\paragraph{Checks for minor allele frequency
(MAF)}\label{checks-for-minor-allele-frequency-maf}

To investigate how many SNPs have MAF\textless{}1\% by first of all
running the `--freq' option in PLINK:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{plink}\NormalTok{ --bfile genstudy_sampleqc --noweb --freq --out genstudy_sampleqc}
\FunctionTok{awk} \StringTok{'$5<0.01'}\NormalTok{ genstudy_sampleqc.frq }\KeywordTok{|} \FunctionTok{wc}\NormalTok{ -l}
\end{Highlighting}
\end{Shaded}

NOTE: Data quality tends to decrease with decreasing MAF. A low MAF
implies rare genotypes which will be seen only a few times in your GWAS
dataset. This in turn implies that there is less information that a
genotype calling algorithm can use to call this genotype, and so calls
are less certain. And the power to detect an association signal
decreases with decreasing MAF. However, if a large MAF threshold is
selected, this implies more SNPs are removed from study which possibly
includes the true causal SNPs. A moderate MAF such as 0.05 is suggested.

\paragraph{Checks for adherence to Hardy-Weinberg
Equilibrium}\label{checks-for-adherence-to-hardy-weinberg-equilibrium}

To generate a list of genotype counts and Hardy-Weinberg test statistics
for each SNP:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{plink}\NormalTok{ --bfile genstudy_sampleqc --noweb --hardy --out genstudy_sampleqc}
\end{Highlighting}
\end{Shaded}

This provides and output file `genstudy.hwe' and the p-value for the
test for Hardy-Weindberg Equilibrium is in column 9 of this file.

To see how many SNPs have HWE p-value\textless{}0.0001 (in controls
``UNAFF'') and pvalue\textless{}0.00001:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{awk} \StringTok{'$3=="UNAFF" && $9<0.0001'}\NormalTok{ genstudy_sampleqc.hwe }\KeywordTok{|} \FunctionTok{wc}\NormalTok{ -l}
\FunctionTok{awk} \StringTok{'$3=="UNAFF" && $9<0.00001'}\NormalTok{ genstudy_sampleqc.hwe }\KeywordTok{|} \FunctionTok{wc}\NormalTok{ -l}
\end{Highlighting}
\end{Shaded}

To apply some extra SNP QC filters to our already sample-QC'd dataset:

\begin{itemize}
\tightlist
\item
  Minor allele frequency (MAF) \textgreater{}0.05 -- option `--maf 0.05'
\item
  SNP genotyping rate \textgreater{}95\% (i.e. \textless{}5\% missing
  genotypes for the SNP, across all individuals) -- option `-- geno
  0.05'
\item
  Hardy-Weinberg p-value \textgreater{}0.0001 in controls -- option `--
  hwe 0.0001'
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{plink}\NormalTok{ --bfile genstudy_sampleqc --noweb --maf 0.05 --geno 0.05 --hwe 0.0001 --make-bed --out genstudy_qc}
\end{Highlighting}
\end{Shaded}

\section{Testing for Associations}\label{testing-for-associations}

In this section, we will undertake an analysis of association:

\begin{itemize}
\tightlist
\item
  first, with no adjustment made for non-genetic variables (univariate
  analysis of association)
\item
  then, with adjustment made for relevant non-genetic variables
  (multiple regression analysis)
\end{itemize}

\subsection{Software and datasets}\label{software-and-datasets-1}

We will use the QC'd PLINK files (generated in the previous section)
ready to run the analyses of association.

To run the analyses of association, we use a software package called
`SNPTEST (Version 2)' -- it's free and available for the analysis of
genome-wide association studies.

To use SNPTEST, input files need to be in a specific format:

\begin{itemize}
\tightlist
\item
  one `.gen' file including genotype data, which includes one row per
  SNP.
\item
  one `.sample' file including phenotypic data. It has three parts (a) a
  header line detailing the names of the columns in the file, (b) a line
  detailing the types of variables stored in each column, and (c) a line
  for each individual including the information for that individual.
\end{itemize}

A `.gen' file includes one row per SNP. The first five entries on each
line should be as follows:

\begin{itemize}
\tightlist
\item
  SNP ID: This entry is usually used to denote the chromosome number.
\item
  rs number: This entry is a number which uniquely identifies the
  genotyped SNP. An rs number is an accession number used by researchers
  and databases to refer to specific SNPs. It stands for `Reference SNP
  cluster ID'.
\item
  Base pair position of the SNP: A value that described the SNP's
  position on the chromosome.
\item
  Allele coded A (see below for further explanation of this): The entry
  here will be A, C, T or G i.e.~corresponding to the four possible
  nucleotides.
\item
  Allele coded B (see below for further explanation of this): The entry
  here will be A, C, T or G i.e.~corresponding to the four possible
  nucleotides.
\end{itemize}

The remaining entries on each row represent the genotype of each
individual at the SNP. Each individual will have three entries,
representing their probability of having the `AA', `AB' and `BB'
genotypes respectively. As we do not have any imputed SNPs in the
current dataset, the probability for each genotype will be either 0 or
1. So, an individual with genotype AA will be coded `1 0 0', with
genotype AB will be coded `0 1 0', and with genotype BB will be coded `0
0 1'. If genotype is missing for an individual, all three entries are
set to `0 0 0'.

It is possible to convert QC'd PLINK files to `.gen' and `.sample'
format using a program called `GTOOL'. We have also removed all
chromosome X SNPs from these files, since in terms of association we are
only interested in SNPs on chromosome 10.

\subsection{Univariate analyses of
association}\label{univariate-analyses-of-association}

To run univariate analyses of association between all SNPs in our
`genstudy\_qc' file and our phenotype `diseased', assuming an additive
mode of inheritance, we would use the following SNPtest command:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{snptest}\NormalTok{ \textbackslash{}}
\NormalTok{-data genstudy_qc.gen genstudy_qc.sample \textbackslash{}}
\NormalTok{-o genstudy_qc_univariate_add.out \textbackslash{}}
\NormalTok{-pheno diseased \textbackslash{}}
\NormalTok{-frequentist 1 \textbackslash{}}
\NormalTok{-method threshold}
\end{Highlighting}
\end{Shaded}

Due to the large number of output columns in
`genstudy\_qc\_univariate\_dom.out' and the large number of SNPs
investigated, it is useful to look at our results in summary format. A
useful way of doing this is by preparing a Manhattan plot to give a
graphical representation of our results. Essentially, this is a plot of
pvalue versus base pair position for each SNP and is a useful way of
identifying significantly associated SNPs.

First of all, it is necessary to re-format our output file into a format
which is appropriate for the shell script.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Add chromosome number to each row (also need to delete final row in
  datafile since includes an additional line of text which is not
  needed)
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{awk} \StringTok{'NR>11\{$1=10; print\}'}\NormalTok{ genstudy_qc_univariate_add.out }\OperatorTok{>}\NormalTok{genstudy_qc_new.out}
\FunctionTok{sed} \StringTok{'$d'}\NormalTok{ genstudy_qc_new.out}\OperatorTok{>}\NormalTok{genstudy_qc_new2.out}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Select only columns of interest, and label them:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{echo} \StringTok{"CHR SNP BP P"} \OperatorTok{>}\NormalTok{ genstudy_qc_Manhattan.txt}\KeywordTok{;}
\FunctionTok{awk} \StringTok{'\{print $1,$2,$4,$42\}'}\NormalTok{ genstudy_qc_new2.out }\OperatorTok{>>}\NormalTok{genstudy_qc_Manhattan.txt}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Create a new file (``genstudy\_qc.unimp.snp'') which lists all
  actually genotyped SNPs.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{awk} \StringTok{' $9==1\{print $2\}'}\NormalTok{ genstudy_qc_univariate_add.out }\OperatorTok{>}\NormalTok{ genstudy_qc.unimp.snp}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Now, the data is prepared for generating the Manhattan plot.
\end{enumerate}

To prepare a Q-Q plot of your results to ensure that there is no
apparent genomic inflation in your results, which would suggest
population substructure:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pvals <-}\StringTok{ }\KeywordTok{read.table}\NormalTok{(}\StringTok{"genstudy_qc_Manhattan.txt"}\NormalTok{, }\DataTypeTok{header=}\NormalTok{T)}
\NormalTok{observed <-}\StringTok{ }\KeywordTok{sort}\NormalTok{(pvals}\OperatorTok{$}\NormalTok{P)}
\NormalTok{lobs <-}\StringTok{ }\OperatorTok{-}\NormalTok{(}\KeywordTok{log10}\NormalTok{(observed))}
\NormalTok{expected <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\KeywordTok{length}\NormalTok{(observed))}
\NormalTok{lexp <-}\StringTok{ }\OperatorTok{-}\NormalTok{(}\KeywordTok{log10}\NormalTok{(expected }\OperatorTok{/}\StringTok{ }\NormalTok{(}\KeywordTok{length}\NormalTok{(expected)}\OperatorTok{+}\DecValTok{1}\NormalTok{)))}
\KeywordTok{pdf}\NormalTok{(}\StringTok{"qqplot.pdf"}\NormalTok{, }\DataTypeTok{width=}\DecValTok{6}\NormalTok{, }\DataTypeTok{height=}\DecValTok{6}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{7}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{7}\NormalTok{), }\DataTypeTok{col=}\StringTok{"red"}\NormalTok{, }\DataTypeTok{lwd=}\DecValTok{3}\NormalTok{, }\DataTypeTok{type=}\StringTok{"l"}\NormalTok{, }\DataTypeTok{xlab=}\StringTok{"Expected (-logP)"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"Observed (-logP)"}\NormalTok{,}
\DataTypeTok{xlim=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{7}\NormalTok{), }\DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{7}\NormalTok{), }\DataTypeTok{las=}\DecValTok{1}\NormalTok{, }\DataTypeTok{xaxs=}\StringTok{"i"}\NormalTok{, }\DataTypeTok{yaxs=}\StringTok{"i"}\NormalTok{, }\DataTypeTok{bty=}\StringTok{"l"}\NormalTok{)}
\KeywordTok{points}\NormalTok{(lexp, lobs, }\DataTypeTok{pch=}\DecValTok{23}\NormalTok{, }\DataTypeTok{cex=}\NormalTok{.}\DecValTok{4}\NormalTok{, }\DataTypeTok{bg=}\StringTok{"black"}\NormalTok{)}
\KeywordTok{dev.off}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

To extract a list of the SNPs giving the lowest p-values:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{awk} \StringTok{' $4 <0.0001'}\NormalTok{ genstudy_qc_Manhattan.txt }\OperatorTok{>}\NormalTok{ lowest_ps_univariate}
\FunctionTok{awk} \StringTok{' $4>0'}\NormalTok{ lowest_ps_univariate }\OperatorTok{>}\NormalTok{ lowest_ps_univariate_new }\CommentTok{#to filter out SNPs were it was not possible to undertake an analysis of association}
\end{Highlighting}
\end{Shaded}

\subsection{Multiple regression
analyses}\label{multiple-regression-analyses}

In the `genstudy.sample' file you will notice four columns to the right
of the phenotype column `diseased'. These are labelled `age', `bmi',
`family' and `sex', and represent four variables believed to be of
potential interest in terms of being associated with our disease.

To decide whether to adjust for each of these variables, we first of all
need to test whether they are univariately associated with our phenotype
`diseased'. As our phenotype is binary, we can use Student's t-test to
test for association with age and bmi, and the chi-square test to test
for association with family history and sex.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data=}\KeywordTok{read.table}\NormalTok{(}\StringTok{"genstudy_qc.sample"}\NormalTok{,}\DataTypeTok{header=}\NormalTok{T)}
\NormalTok{data=data[}\OperatorTok{-}\DecValTok{1}\NormalTok{,]}
\KeywordTok{attach}\NormalTok{(data)}
\NormalTok{res_age=}\KeywordTok{t.test}\NormalTok{(}\KeywordTok{as.numeric}\NormalTok{(age)}\OperatorTok{~}\NormalTok{diseased)}
\NormalTok{res_bmi=}\KeywordTok{t.test}\NormalTok{(}\KeywordTok{as.numeric}\NormalTok{(bmi)}\OperatorTok{~}\NormalTok{diseased)}
\NormalTok{res_family=}\KeywordTok{chisq.test}\NormalTok{(family,diseased)}
\NormalTok{res_sex=}\KeywordTok{chisq.test}\NormalTok{(sex,diseased)}
\NormalTok{res_age}\OperatorTok{$}\NormalTok{p.value}
\NormalTok{res_bmi}\OperatorTok{$}\NormalTok{p.value}
\NormalTok{res_family}\OperatorTok{$}\NormalTok{p.value}
\NormalTok{res_sex}\OperatorTok{$}\NormalTok{p.value}
\end{Highlighting}
\end{Shaded}

To now run our SNP association analyses using a multiple regression
approach, we can add the element `-cov\_names' followed by the clinical
variable names to your command (assuming that we have decided to adjust
for all clinical variables giving p\textless{}0.05 in our univariate
analysis, we only have to adjust for age and family history):

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{snptest}\NormalTok{ \textbackslash{}}
\NormalTok{-data genstudy_qc.gen genstudy_qc.sample \textbackslash{}}
\NormalTok{-o genstudy_qc_adjusted_add.out \textbackslash{}}
\NormalTok{-pheno diseased \textbackslash{}}
\NormalTok{-frequentist 1 \textbackslash{}}
\NormalTok{-method threshold \textbackslash{}}
\NormalTok{-cov_names age family}
\end{Highlighting}
\end{Shaded}

To prepare a Manhattan plot and QQ-plot of the results from your
multiple regression analysis by adjusting the code used for plotting the
univariate results:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{awk} \StringTok{'NR>11\{$1=10; print\}'}\NormalTok{ genstudy_qc_adjusted_add.out }\OperatorTok{>}\NormalTok{ genstudy_qc_new.out}
\FunctionTok{sed} \StringTok{'$d'}\NormalTok{ genstudy_qc_new.out }\OperatorTok{>}\NormalTok{ genstudy_qc_new2.out}
\FunctionTok{tail}\NormalTok{ -n +2 genstudy_qc_new2.out }\OperatorTok{>}\NormalTok{ genstudy_qc_new3.out}
\BuiltInTok{echo} \StringTok{"CHR SNP BP P"} \OperatorTok{>}\NormalTok{ genstudy_qc_adj_Manhattan.txt}
\FunctionTok{awk} \StringTok{'\{print $1,$2,$4,$42\}'}\NormalTok{ genstudy_qc_new3.out }\OperatorTok{>>}\NormalTok{ genstudy_qc_adj_Manhattan.txt}
\FunctionTok{awk} \StringTok{'$9==1\{print $2\}'}\NormalTok{ genstudy_qc_adjusted_add.out }\OperatorTok{>}\NormalTok{ genstudy_qc.adj.snp}
\FunctionTok{sh}\NormalTok{ QuickManhattan.sh}
\end{Highlighting}
\end{Shaded}

To obtain a list of the lowest p-values from this analysis, and compare
them with your lowest pvalues from the univariate analysis:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{awk} \StringTok{' $4 <0.0001'}\NormalTok{ genstudy_qc_adj_Manhattan.txt }\OperatorTok{>}\NormalTok{ lowest_ps_adjusted}
\FunctionTok{awk} \StringTok{'$4>0'}\NormalTok{ lowest_ps_adjusted }\OperatorTok{>}\NormalTok{ lowest_ps_adjusted_new}
\FunctionTok{cat}\NormalTok{ lowest_ps_adjusted_new}
\end{Highlighting}
\end{Shaded}

\section{Population structure in
GWAS}\label{population-structure-in-gwas}

\subsection{Testing for association under an additive
model}\label{testing-for-association-under-an-additive-model}

To test for association under an additive model using the PLINK command,
but without adjustment for any covariates:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{plink}\NormalTok{ --noweb --bfile EGCUT.clean --logistic --ci 0.95 --out plink.additive}
\end{Highlighting}
\end{Shaded}

To investigate the impact of population structure by generating a QQ
plot of observed p-values against those that would be expected under the
null hypothesis of no association. In the absence of population
structure, we would expect most points to lie on the y=x line:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{R}\NormalTok{ --vanilla --slave --args plink.additive.assoc.logistic ADD qq_1.pdf }\OperatorTok{<}\NormalTok{ qqPlot.R}
\end{Highlighting}
\end{Shaded}

\subsection{Performing multi-dimensional
scaling}\label{performing-multi-dimensional-scaling}

Multi-dimensional scaling in PLINK is performed in three steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Identifying a subset of ``independent'' common SNPs (minor allele
  frequency of at least 5\%) that are not in linkage disequilibrium with
  each other.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{plink}\NormalTok{ --noweb --bfile EGCUT.clean --maf 0.05 --indep-pairwise 50 5 0.05}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Calculating the relatedness between each pair of individuals using the
  set of independent SNPs which is a measure of genetic similarity.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{plink}\NormalTok{ --noweb --bfile EGCUT.clean --extract plink.prune.in --Z-genome}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Perform multi-dimensional scaling using the relatedness matrix.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{plink}\NormalTok{ --noweb --bfile EGCUT.clean --read-genome plink.genome.gz --cluster --mds-plot 2}
\end{Highlighting}
\end{Shaded}

Note that the option --Z-genome produces a compressed genome file and
saves on storage. The option --mds-plot specifies how many eigenvectors
from the multi-dimensional scaling to summarise in the output file. The
eigenvectors are output to the file plink.mds.

The output file has one row per individual, and provides the identifier
used in the PLINK family file, together with the first two eigenvectors
(columns C1 and C2). You can plot the first two eigenvectors against
each other using the command:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{R}\NormalTok{ --vanilla --slave --args plink.mds EGCUT.clean.fam mds.pdf }\OperatorTok{<}\NormalTok{ mds.R}
\end{Highlighting}
\end{Shaded}

In the output plot, each point corresponds to an individual.

\subsection{Testing for association under an additive model with
adjustment for
confounding}\label{testing-for-association-under-an-additive-model-with-adjustment-for-confounding}

To account for population structure in our analysis, we can repeat our
test of association under and additive model, but this type adjusting
for eigenvectors from the multi-dimensional scaling as covariates:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{plink}\NormalTok{ --noweb --bfile EGCUT.clean --logistic --covar plink.mds --covar-name C1,C2 --ci 0.95 --out plink.additive.mds}
\end{Highlighting}
\end{Shaded}

To investigate the impact of population structure that is not accounted
for by the first two eigenvectors from multi-dimensional scaling by
generating a QQ plot with the command:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{R}\NormalTok{ --vanilla --slave --args plink.additive.mds.assoc.logistic ADD qq_2.pdf }\OperatorTok{<}\NormalTok{ qqPlot.R}
\end{Highlighting}
\end{Shaded}

To obtain association summary statistics for the SNP:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{grep}\NormalTok{ SNPID plink.additive.mds.assoc.logistic}
\end{Highlighting}
\end{Shaded}

\section{Phasing and Imputation}\label{phasing-and-imputation}

By utilising genotype imputation methods, we can investigate whether
there are neighbouring SNPs to a particular SNP that generated a
significant p-value, also sharing evidence of association with the
phenotype. The focus in this section will be on imputing genotypes
within close proximity of a significant SNP, and then to re-run the
association analyses on the imputed dataset.

\subsection{Pre-phasing and
imputation}\label{pre-phasing-and-imputation}

Prior to undertaking genotype imputation, it is necessary to
``pre-phase'' the genotype data to convert it into haplotype form.

Doing this affords more efficient imputation as the reference panels on
which we base our imputation are all in haplotype form as well.
Haplotypes span across many locus so it is important that when focusing
in on a certain region of the genome, (for example the region
surrounding a significant SNP as we are in this practical), that we
allow a sufficient buffer zone around the region. If we didn't allow
this buffer zone then we could interrupt haplotype construction with the
locus of interest!

To undertake the pre-phasing we use a software package called SHAPEIT2
(\url{https://mathgen.stats.ox.ac.uk/genetics_software/shapeit/shapeit.htm}).

The causal SNP we found before had base pair position 64803579 and so a
sensible region to pre-phase might be from 63Mb to 68Mb, which will
include the immediate region surrounding the causal SNP, plus a buffer
region at either end.

To pre-phase the ``genstudy\_pp'' dataset, we can run the following
SHAPEIT2 command:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{shapeit}\NormalTok{ --input-ped genstudy_pp.ped genstudy_pp.map --input-map chr10.map --output-max genstudy.ph --thread 2 --input-from 63000000 --input-to 68000000}
\end{Highlighting}
\end{Shaded}

Once we have pre-phased our data, we can undertake genotype imputation
and to do this we will use a software package called ``IMPUTE2''
(\url{https://mathgen.stats.ox.ac.uk/impute/impute_v2.html}).

After running SHAPEIT2 on the ``genstudy\_pp'' dataset, we obtained a
``genstudy.ph.haps'' datafile which contained phased haplotypes for our
specified region. We can now run IMPUTE2 on this .haps datafile to
impute genotypes at SNPs not originally genotyped, but included on our
chosen reference panel. For the purpose of this practical, we will use
the reference panel from the 1000 Genomes Project
(\url{http://www.1000genomes.org}). To do this, we can use the following
IMPUTE2 command:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{impute2}\NormalTok{ -use_prephased_g -known_haps_g genstudy.ph.haps -m chr10.map -h chr10.haps.gz -l chr10.leg.gz -int 64300000 65300000 -Ne 20000 -buffer 500 -o genstudy_imp.gen}
\end{Highlighting}
\end{Shaded}

Please note that the interval we specify under the option ``-int''
(64300000 to 65300000) is narrower than the one we specified in the
SHAPEIT2 command. This is because the imputation process will produce
genotypes rather than haplotypes. Therefore we can narrow down to a 1Mb
region of interest.

After the Imputation has run, in the screen output there is a section
called ``Imputation accuracy assessment''. The interval here denotes the
probability range at which a SNP genotype has been called and the
concordance with the patient genotypes. These figures can be improved by
increasing the number of patients in the study or with better match
reference haplotypes.

\subsection{Re-running our univariate analyses of
association}\label{re-running-our-univariate-analyses-of-association}

Previously, when we were dealing only with actually genotyped SNPs, our
.gen file (``genstudy\_qc.gen'') included the first five compulsory
columns, followed by three columns for each genotyped SNP. The three
columns represented an individual's probability of having the ``AA'',
``AB'' and ``BB'' genotypes respectively. As we did not have any imputed
SNPs, the probability for each genotype was either 0 or 1. However, now
that we are dealing with imputed SNPs, the probability for each genotype
can be any number between 0 and 1, representing our genotype
uncertainty.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{ -n 1 genstudy_imp.gen}
\end{Highlighting}
\end{Shaded}

Please, notice that there are number others than 0 and 1 in the columns
representing genotype probabilities for the SNPs.

To run univariate analyses of association on a dataset which includes
imputed genotypes, we can again use the SNPTest programme. However, the
element ``method'' within the SNPTest command needs to be specified
differently. The ``method'' element controls the way in which genotype
uncertainty is accounted for in the analyses of association, bearing in
mind that imputation can only provide a `best estimate' of what the true
genotype at a SNP would be.

\begin{itemize}
\tightlist
\item
  threshold: Assumes a particular genotype at a SNP only if accuracy of
  the genotype call is above a given threshold. The calling threshold is
  controlled by the flag -call\_thresh and the default calling threshold
  is 0.9.
\item
  expected: Uses expected genotype counts, also known as genotype
  dosages, in the analyses of association.
\item
  score: Uses a missing data likelihood score test in the analyses of
  association.
\item
  ml: Again uses missing data likelihood approach, but with multiple
  Newton-Raphson iterations to estimate the parameters in the missing
  data likelihood.
\item
  em: Again uses missing data likelihood approach, but with an EM
  algorithm to estimate the parameters in the missing data likelihood.
\end{itemize}

So, to run a univariate analysis of association on our imputed dataset,
assuming the threshold method we would use the following command:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{snptest}\NormalTok{ -data genstudy_imp.gen genstudy_qc.sample -o genstudy_qc_univariate_add.imp.out -pheno diseased -frequentist 1 -method threshold}
\end{Highlighting}
\end{Shaded}

The SNPtest output file ``genstudy\_qc\_univariate\_add.imp.out'' can
now be scrutinised in much the same way as we did for the output file
initially -- i.e.~we can produce Manhattan and QQ plots of the resulting
p-values -- except for one additional and important QC step which needs
to occur post-imputation. This QC step involves filtering out SNPs with
poor imputation accuracy. The SNPtest output file has a column labelled
``INFO'', which contains, for each SNP, a number indicating how
accurately it has been imputed. The value of INFO will range from 0 (no
certainty in imputation) to 1 (perfect accuracy in imputation/actually
genotyped SNP). Poorly imputed SNPs are likely to contribute to false
positive results whilst removing a large number of imputed SNPs will
distort the overall results. Therefore, as a QC step we need to identify
an appropriate, yet not too stringent, threshold for the level of
accuracy that is allowed. Unfortunately, there is no exact science to
choosing the threshold; the SNPtest website
(\url{https://mathgen.stats.ox.ac.uk/impute/impute_v2.html}) views INFO
thresholds between 0.3 and 0.5 as reasonable. For this analysis we will
utilise an INFO threshold of 0.4.

This additional QC step can be implemented very easily by:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{awk} \StringTok{'NR>11&&$9>0.4\{$1=10;print\}'}\NormalTok{ genstudy_qc_univariate_add.imp.out }\OperatorTok{>}\NormalTok{genstudy_qc_new.imp.out}
\end{Highlighting}
\end{Shaded}

\chapter{AWS}\label{aws}

\section{AWS S3}\label{aws-s3}

\subsection{Accessing AWS using the AWS Command Line
Interface}\label{accessing-aws-using-the-aws-command-line-interface}

The AWS Command Line Interface (or AWS CLI) is an open source tool that
enables you to interact with the AWS services from your command-line
Shell. AWS CLI tool is already installed in our server, and it is very
simple to use. Below are described the most commonly used interactions
with AWS.

\subsubsection{Configure the AWS CLI
service}\label{configure-the-aws-cli-service}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  To configure the service, please type ``aws configure'' on the
  console. This command is interactive, so the service will prompt you
  four times to enter some config information. Below an example (all
  RytenLab members have to ask me to send them their secret AWS
  credentials):
\end{enumerate}

\begin{verbatim}
$ aws configure 

AWS Access Key ID [None]: your_access_key 
AWS Secret Access Key [None]: your_secret_key 
Default region name [None]: eu-west-2 
Default output format [None]: json 
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  To check whether the connection with AWS has been correctly
  stablished, we can type the following command to list all our current
  buckets.
\end{enumerate}

\begin{verbatim}
$ aws s3 ls 
\end{verbatim}

\subsubsection{Upload a single file to
AWS}\label{upload-a-single-file-to-aws}

Let's suppose we have a bucket on AWS called `my-bucket'. Let's also
suppose you have a file called `myfile.txt' stored in your local that
you would like to upload to AWS. To upload the file `myfile.txt' to the
bucket `my-bucket':

\begin{verbatim}
$ aws s3 cp myfile.txt s3://my-bucket/ 
\end{verbatim}

\subsubsection{Download a single file from
AWS}\label{download-a-single-file-from-aws}

To download the file `myfile.txt' from the `s3://my-bucket/' AWS bucket
into your local folder:

\begin{verbatim}
$ aws s3 cp s3://my-bucket/myfile.txt ./my_local_folder 
\end{verbatim}

\subsubsection{Upload multiple files to
AWS}\label{upload-multiple-files-to-aws}

To upload multiple files, we can use the sync command. The \textbf{sync}
command syncs objects under a specified local folder to files in a AWS
bucket by uploading them to AWS.

\begin{verbatim}
$ aws s3 sync my_local_folder/ s3://my-bucket/ 
\end{verbatim}

\subsubsection{Download multiple files from
AWS}\label{download-multiple-files-from-aws}

To download multiple files from an AWS bucket to your local folder, we
can also use the \textbf{sync} command by changing the order of the
parameters.

\textbf{\emph{Please, be aware the costs associated with downloading
files correspond to \$0.090 per GB - first 10 TB / month data transfer
out beyond the global free tier.}}

\begin{verbatim}
$ aws s3 sync s3://my-bucket/my_remote_folder/ ./my_local_folder 
\end{verbatim}

\subsection{Checking AWS file
integrity}\label{checking-aws-file-integrity}

Considering the example used in the previous section, let's check the
integrity of the local folder `./my\_local\_folder' matches with the
integrity of the remote AWS folder `s3://my-bucket/my\_local\_folder/'.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  First, clone the `aws-s3-integrity-check' repo.
\end{enumerate}

\begin{verbatim}
$ git clone https://github.com/SoniaRuiz/aws-s3-integrity-check.git
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Clone the `s3md5' repo.
\end{enumerate}

\begin{verbatim}
$ git clone https://github.com/antespi/s3md5.git
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Move the s3md5 folder within the aws-s3-integrity-check folder:
\end{enumerate}

\begin{verbatim}
$ mv ./s3md5 ./aws-s3-integrity-check
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Next, grant execution access permission to he s3md5 script file.
\end{enumerate}

\begin{verbatim}
$ chmod 755 ./aws-s3-integrity-check/s3md5/s3md5
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  The aws-s3-integrity-check folder should look similar to the
  following:
\end{enumerate}

\begin{verbatim}
total 16
-rw-r--r-- 1 your_user your_group 3573 date README.md
-rwxr-xr-x 1 your_user your_group 3301 date aws_check_integrity.sh
drwxr-xr-x 2 your_user your_group 4096 date s3md5
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  Execute the script `aws\_check\_integrity.sh' following the this
  structure: \textbf{aws\_check\_integrity.sh `local\_path'
  `bucket\_name' `bucket\_folder'}
\end{enumerate}

\begin{verbatim}
$ aws_check_integrity.sh ./my_local_folder my-bucket my_local_folder/
\end{verbatim}

\subsection{Creating a new AWS bucket}\label{creating-a-new-aws-bucket}

To create a new AWS bucket, we recommend using the following
configuration:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Region: EU London
\item
  Block all public access: enabled
\item
  Bucket Versioning: enable
\item
  Tags:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    Key = ``data-owner'' / Value = ``your name''
  \item
    Key = ``data-origin'' / Value = ``the origin of the data in one word
    (i.e.~the name of a project, the name of a server)''
  \end{enumerate}
\item
  Default encryption: enable - Amazon S3 key (SSE-S3)
\item
  Advanced settings

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    Object Lock: enable/disable, depending on your type of data (more
    info
    \href{https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lock.html}{here})
  \end{enumerate}
\end{enumerate}

\section{AWS EC2}\label{aws-ec2}

All commands here shown have been used to configure an EC2 virtual
instance under the AWS free-tier.

\subsection{Generating an EC2
instance}\label{generating-an-ec2-instance}

To generate an EC2 instance under the free-tier using an Ubuntu 20.04
LTS image, please follow the next steps:

\begin{itemize}
\item
  Log in on the
  \href{https://eu-west-2.console.aws.amazon.com/console/home?region=eu-west-2}{AWS
  Console} using your AWS account.
\item
  Click on Services --\textgreater{} EC2 --\textgreater{} Instances.
\item
  Click on the ``Launch Instances'' button. It is an orange button
  located on the upper right-hand side of the web page.

  \begin{itemize}
  \tightlist
  \item
    Step 1: Choose an Amazon Machine Image (AMI). Click on the ``Free
    tier only'' checkbox (located on the left-hand side menu) and select
    an AIM from the list. In this example I am going to use the ``Ubuntu
    Server 20.04 LTS (HVM), SSD Volume Type'' AMI.
  \item
    Step 2: Choose an Instance Type. Select ``t2.micro'' (free tier
    eligible).
  \item
    Step 3: Configure Instance Details. Leave all options by default and
    click ``Next''.
  \item
    Step 4: Add Storage. Change the size of your disk from 8GB to 25GB.
    Free tier allows to get up to 30 GB of (SSD) disk storage.
  \item
    Step 5: Add Tags. Add any tags or just leave it by default and click
    Next.
  \item
    Step 6: Configure Security Group. We will need to add an additional
    HTTP rule and two custom TCP rules to set the ports to 3838 (for
    RServer to run) and 8787 (for RStudio to run). See ``Step 5:
    Configuring the Security Group'' section from this
    \href{https://jagg19.github.io/2019/08/aws-r/}{post} for more
    details.
  \item
    Step 7: Review and Launch. Review the info and click ``Launch''.
  \item
    Step 8: Select or Create Key Pair. From the dropdown, choose
    ``Create a new key pair''. Name the .pem file and store it securily
    in your local computer. You will need later to connect to your EC2
    instance.
  \end{itemize}
\end{itemize}

\subsection{Connecting to an EC2
instance}\label{connecting-to-an-ec2-instance}

SSH login to your EC2 instance:

\begin{itemize}
\tightlist
\item
  From your AWS Console. Click on ``Services --\textgreater{} EC2
  --\textgreater{} Instances''. Check your newly created instance is
  running.
\item
  Click on the ``Connect'' button and select the ``SSH Client'' tab.
\item
  Follow the instructions indicated there to connect to your EC2
  instance from an SSH client (I normally use MobaXterm).
\end{itemize}

\subsection{Installing/Running RStudio on an
EC2}\label{installingrunning-rstudio-on-an-ec2}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## More info: https://jagg19.github.io/2019/08/aws-r/}

\CommentTok{## Update the Ubuntu repos }
\CommentTok{# update indices}
\FunctionTok{sudo}\NormalTok{ apt update -qq}
\CommentTok{# install two helper packages we need}
\FunctionTok{sudo}\NormalTok{ apt install --no-install-recommends software-properties-common dirmngr}
\CommentTok{# add the signing key (by Michael Rutter) for these repos}
\CommentTok{# To verify key, run gpg --show-keys /etc/apt/trusted.gpg.d/cran_ubuntu_key.asc }
\CommentTok{# Fingerprint: 298A3A825C0D65DFD57CBB651716619E084DAB9}
\FunctionTok{sudo}\NormalTok{ wget -qO- https://cloud.r-project.org/bin/linux/ubuntu/marutter_pubkey.asc }\KeywordTok{|} \FunctionTok{sudo}\NormalTok{ tee -a /etc/apt/trusted.gpg.d/cran_ubuntu_key.asc}
\CommentTok{# add the R 4.0 repo from CRAN -- adjust 'focal' to 'groovy' or 'bionic' as needed}
\FunctionTok{sudo}\NormalTok{ add-apt-repository }\StringTok{"deb https://cloud.r-project.org/bin/linux/ubuntu }\VariableTok{$(}\ExtensionTok{lsb_release}\NormalTok{ -cs}\VariableTok{)}\StringTok{-cran40/"}


\CommentTok{# Update ubuntu package repo, to get latest R}
\FunctionTok{sudo}\NormalTok{ apt update}

\CommentTok{# Install R}
\FunctionTok{sudo}\NormalTok{ apt -y install r-base r-base-dev}

\CommentTok{# Install shiny before shiny-server}
\FunctionTok{sudo}\NormalTok{ R -e }\StringTok{"install.packages('shiny')"}


\CommentTok{# Install debian package manager, gdebi}
\FunctionTok{sudo}\NormalTok{ apt install gdebi-core}


\CommentTok{# Install Shiny Server (https://www.rstudio.com/products/shiny/download-server/)}
\FunctionTok{sudo}\NormalTok{ wget https://download3.rstudio.org/ubuntu-14.04/x86_64/shiny-server-1.5.17.973-amd64.deb}
\FunctionTok{sudo}\NormalTok{ gdebi shiny-server-1.5.17.973-amd64.deb}
\FunctionTok{sudo}\NormalTok{ rm shiny-server-1.5.17.973-amd64.deb}


\CommentTok{# Dependencies for R packages like RMariaDB, devtools, tidyverse, sparklyr. Please run seperate.}
\FunctionTok{sudo}\NormalTok{ apt -y install libcurl4-openssl-dev }
\FunctionTok{sudo}\NormalTok{ apt -y install libssl-dev libxml2-dev libmariadbclient-dev build-essential libcurl4-gnutls-dev}


\CommentTok{# Install RStudio (https://www.rstudio.com/products/rstudio/download-server/)}
\FunctionTok{wget}\NormalTok{ https://download2.rstudio.org/server/bionic/amd64/rstudio-server-2021.09.2-382-amd64.deb}
\FunctionTok{sudo}\NormalTok{ gdebi rstudio-server-2021.09.2-382-amd64.deb}
\FunctionTok{sudo}\NormalTok{ rm rstudio-server-2021.09.2-382-amd64.deb}


\CommentTok{# Install some useful R Packages }
\FunctionTok{sudo}\NormalTok{ R -e }\StringTok{"install.packages('RCurl', repos='http://cran.rstudio.com')"}
\FunctionTok{sudo}\NormalTok{ R -e }\StringTok{"install.packages('devtools', repos='http://cran.rstudio.com')"}
\FunctionTok{sudo}\NormalTok{ R -e }\StringTok{"install.packages('tidyverse')"}
\FunctionTok{sudo}\NormalTok{ R -e }\StringTok{"install.packages('RMariaDB')"}


\CommentTok{# Add user info to login RStudio}
\FunctionTok{sudo}\NormalTok{ adduser user_name}
\CommentTok{#Add rstudio to sudo group}
\FunctionTok{sudo}\NormalTok{ usermod -aG sudo user_name}


\CommentTok{# Install Java and reconfigure in R for RStudio use}
\FunctionTok{sudo}\NormalTok{ apt -y install default-jdk}
\FunctionTok{sudo}\NormalTok{ R CMD javareconf}
\CommentTok{# Change permissions for R library}
\FunctionTok{sudo}\NormalTok{ chmod 777 -R /usr/local/lib/R/site-library}


\CommentTok{#Restart rstudio-server to incorporate changes made in rserver.conf}
\FunctionTok{sudo}\NormalTok{ rstudio-server restart}
\end{Highlighting}
\end{Shaded}

\subsection{Access RStudio through your
browser}\label{access-rstudio-through-your-browser}

\begin{itemize}
\tightlist
\item
  From your AWS Console, click on ``Services --\textgreater{} EC2
  --\textgreater{} Instances''.
\item
  Click on the checkbox located next to the EC2 instance to select it.
\item
  At the bottom of the web page it should have appeared a section with
  info from the selected instance.
\item
  Copy the URL indicated under the ``Public IPv4 DNS'' section.
\item
  Add the port 8787 to the URL
  (i.e.~\url{http://}\ldots{}\ldots{}:8787),
\item
  Paste the URL on your browser. RStudio should now appear on your
  browser.
\end{itemize}

\section{AWS List of Cloud Services}\label{aws-list-of-cloud-services}

\begin{table}

\caption{\label{tab:unnamed-chunk-84}list of AWS cloud services.}
\centering
\begin{tabular}[t]{ll}
\toprule
name & description\\
\midrule
Amazon API Gateway & Service to develop APIs. These APIs will act as the front door for applications to access the data from the backend services behind the API.\\
Amazon Athena & SQL to analyse data stored in S3.\\
Amazon Aurora & MySQL-Compatible    Relational database for the cloud.\\
Amazon Aurora & PostgreSQL-Compatible   Relational database for the cloud.\\
Amazon Bracket & Quantum computing service.\\
\addlinespace
Amazon Carrier & A Carrier IP address is the address that I will assign to a network interface (for example an EC2 instance).\\
Amazon Chime & To let users meet and chat online.\\
Amazon CoudWatch & Monitoring and management service providing insights for AWS.\\
Amazon Code Guru Reviewer & Reviewer    Service that uses program analysis and machine learning to detect potential defects that are difficult for developers to detect.\\
Amazon Cognito & To add user sign-up, sign-in and access control to my web and mobile apps.\\
\addlinespace
Amazon Comprehend & For text processing and analysis.\\
Amazon Comprehend Medical & HIPAA-eligible natural language processing (NLP)\\
Amazon DocumentDB & Document database service\\
Amazon DynamoDB & Is a key-value and document database that delivers single-digit millisecond performance at any scale.\\
Amazon EC2 & Compute platform\\
\addlinespace
Amazon EC2 Dedicated Hosts & Allows you to use your eligible software licences on Amazon EC2.\\
Amazon EKS & Amazon Elastic Kubernetes Service\\
Amazon Elastic Block Store (EBS) & Allows you to create persistent block storage volumes and attach them to Amazon EC2 instances.\\
Amazon Elastic Container Registry (ECR) & To store, manage, and deploy Docker container images.\\
Amazon Elastic File System (EFS) & Provides a file system for use with AWS Cloud services.\\
\addlinespace
Amazon Elastic Graphics & Allows you to attach low-cost graphics acceleration to EC2 instances.\\
Amazon Elastic IP   Static & IPv4 for dynamic cloud computing.\\
Amazon Elastic Transcoder & Is media transcoding in the cloud. Designed to convert (or “transcode”) media files from their source format into versions that will playback on devices like smartphones.\\
Amazon ElastiCache & Build data-intensive apps or improve the performance of your existing apps by retrieving data from high throughput and low latency in-memory data stores.\\
Amazon Elasticsearch Service & To operate Elasticsearch at scale with zero downtime.\\
\addlinespace
Amazon EMR & Industry-leading cloud big data platform for processing vast amounts of data using open-source tools. EMR makes easy to set up, operate, and scale your big-data environments by automating time-consuming tasks like provisioning capacity and tuning clusters. To run petabyte-scale analysis at less than half of the cost of traditional on-premises solutions.\\
Amazon FSx for Lustre & Integrated with S3, is designed for fast processing of workloads.\\
Amazon FSx for Windows File Server & To move your Windows based applications that require file storage to AWS.\\
Amazon GuardDuty & Is a threat detection service that monitors for malicious activity to protect your AWS accounts and data stored in Amazon S3.\\
Amazon Kinesis Data Analytics & To analyse streaming data (for responding to your business and customer needs in real time)\\
\addlinespace
Amazon Kinesis Data Firehose & To only pay for the volume of data ingested into the service.\\
Amazon Kinesis Data streams & Data streaming service.\\
Amazon Lookout for Vision & Machine learning (ML) service that spots defects and anomalies in visual representations using computer vision (CV).\\
Amazon Managed Streaming for Apache Kafka (MSK) & To easily run applications that use Apache Kafka\\
Amazon MQ & To set up and operate message brokers in the cloud.\\
\addlinespace
Amazon Neptune & Graph database service to build and run applications that work with highly connected datasets.\\
Amazon Polly & Service that turns text into lifelike speech.\\
Amazon QuickSight & Business intelligence service to deliver insights to everyone in your organization.\\
Amazon RDS for MariaDB & To set up, operate and scale MariaDB database deployments in the cloud.\\
Amazon RDS for MySQL & ''\\
\addlinespace
Amazon RDS for Oracle & \\
Amazon RDS for PostgreSQL & \\
Amazon RDS for SQL server & \\
Amazon Redshift & Petabyte-scale data warehouse\\
Amazon Rekognition & Images and video analysis recognition; to identify objects, people, text, scenes, etc. Also, to detect inappropriate content.\\
\addlinespace
Amazon Route 53 & DNS web service\\
Amazon S3 Glacier & \\
Amazon SageMaker & To manage costs associated to ML instances\\
Amazon SageMaker Ground Truth & To build training datasets for machine learning.\\
Amazon Simple Email Service & Cloud-based email sending service.\\
\addlinespace
Amazon Simple Notification Service & Amazon Simple Notification Service\\
Amazon Simple Queue Service (SQS) & Amazon Simple Queue Service (SQS)\\
Amazon S3 & Amazon Simple Storage Service (S3).\\
Amazon Simple Workflow Service (SWF) & Amazon Simple Workflow Service (SWF)\\
\bottomrule
\end{tabular}
\end{table}

\chapter{Docker}\label{docker}

\section{Introduction}\label{introduction}

Nowadays, almost all companies and scientific labs which work with data
are also becoming software producers. In some cases, the main reason for
this software development is to publish effective scientific work. In
other cases, its final aim might be just putting together a set of
related functions that work for a similar final objective. However, in
both cases, the final software product becomes highly shareable.

This characteristic, the shareability of the software produced, might
seem very straightforward but in reality, it can turn into a scaring
daunting task to be solved. Why? Because of the heterogeneity of all
different platforms, operating systems, dependencies, versioning and so
on that our software product makes use of. Please, be aware that prior
to its first release, our software product might have been tested in a
limited number of PC stations with determined characteristics that can
be extremely different from any other potential user's PC station around
the world.

All these reasons make \href{https://www.docker.com/}{Docker Platform}
really interesting and useful for both software producers and consumers.

\section{What is docker?}\label{what-is-docker}

\href{https://www.docker.com/}{Docker} is a software platform created in
2013 by \href{https://en.wikipedia.org/wiki/Docker,_Inc.}{Docker, Inc.}
which its main objective is to \emph{build, share, and run any app
anywhere}, independently of the platform and environment where it is
executed.

But what does this definition mean? It means that you admiringly can
forget about dependencies, libraries, compatibility, etc. to make the
app run correctly. To some extent, you could think Docker as a
\emph{black box}, as a \textbf{snapshot} of the developer's laptop where
she or he developt the software you are about to make use of. A snapshot
of the precise moment when the developer decided to release the
software. Thus, everything you need to run the app is already there,
installed and configured inside the Docker object, ready to be used,
with no compatibility issues at all.

\section{Images and containers}\label{images-and-containers}

Working with Docker, there are two main concepts you are going to hear
about constantly: images and containers. An \textbf{image} is the
virtual file or template that contains the whole set of instructions to
build a container. An image is the raw Docker file you will directly
download from \href{https://hub.docker.com/}{Docker Hub} developer's
repository to your local. On the other hand, a \textbf{container} is the
executable object directly generated from the image. A container will be
the virtual object that represents the snapshot of the app developer's
laptop.

In summary, the image is the virtual file that contains the raw
instructions to build the executable app, and the executable app is the
container itself.

\section{Docker Hub}\label{docker-hub}

\href{https://hub.docker.com/}{Docker Hub} is an online platform that
allows creating individual Docker repositories. A Docker repository is a
personal account space where you can store one or more versions of the
same Docker image, which are represented by tags.

Let us focus on the following image obtained from the
\href{https://hub.docker.com/_/ubuntu?tab=tags}{Ubuntu Docker
Repository}:

\section{Useful Commands}\label{useful-commands}

This page contains a list with some of the most common commands of
Docker.

To download a Docker image from Docker Hub:

\begin{verbatim}
$ sudo docker push repository/name:tag
\end{verbatim}

To run the image \emph{name:tag}. The flag \textbf{--rm} indicates to
remove the container after stopping the image; whereas the flag
\textbf{-p} indicates the port on which we want to expose the execution
of the image:

\begin{verbatim}
$ sudo docker run --rm -p 8500:80 name:tag
\end{verbatim}

To list all docker images that are available in our local:

\begin{verbatim}
$ sudo docker images
\end{verbatim}

To remove the image \emph{image\_name}:

\begin{verbatim}
$ sudo docker image rm image_name
\end{verbatim}

To remove all orphaned images:

\begin{verbatim}
$ sudo docker rmi $(sudo docker images -f dangling=true -q)
\end{verbatim}

To list all current containers:

\begin{verbatim}
$ sudo docker ps
\end{verbatim}

To list all stopped containers:

\begin{verbatim}
$ sudo docker ps -a 
\end{verbatim}

To remove all orphaned containers:

\begin{verbatim}
$ sudo docker rm $(sudo docker ps -a -q)
\end{verbatim}

To enter inside a container in execution:

\begin{verbatim}
$ sudo docker exec -it name_container /bin/sh
\end{verbatim}

\section{Docker \& CoExp Web
Application}\label{docker-coexp-web-application}

This tutorial contains the instructions to install a local version of
the CoExp Webpage by making use of the Docker technology. All the
commands shown in this document have been tested in a Ubuntu18.04
machine.

\subsection{Software requirements}\label{software-requirements}

Before downloading the CoExp Docker images, we first need to prepare the
environment for the correct execution of Docker.

Thus, let's download/fetch new versions of the packages that are already
installed in our Linux machine (all these commands have been tested in a
Ubuntu18.04 machine):

\begin{verbatim}
$ sudo apt update
$ sudo apt upgrade
\end{verbatim}

\emph{curl} is a tool used to transfer data. We will make use of it
later when we download the CoExp Docker images. To install \emph{curl}
in the machine:

\begin{verbatim}
$ sudo apt install curl
$ sudo curl --version
\end{verbatim}

Now, we are ready to install the Docker technology in the machine. So,
let's download it:

\begin{verbatim}
$ sudo apt install docker.io
\end{verbatim}

Once the Docker installation has finished, we can enable and start it.
The last instruction \emph{sudo docker --version} will return the
current Docker version installed:

\begin{verbatim}
$ sudo systemctl start docker
$ sudo systemctl enable docker
$ sudo docker --version
\end{verbatim}

Finally, we need to install Docker-compose (more info
\href{https://www.digitalocean.com/community/tutorials/how-to-install-docker-compose-on-ubuntu-18-04}{here}).
Docker-compose is a brach of the Docker technology, which allows
communicating different Docker images between them:

\begin{verbatim}
$ sudo curl -L https://github.com/docker/compose/releases/download/1.21.2/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose
$ sudo chmod +x /usr/local/bin/docker-compose
$ docker-compose --version
\end{verbatim}

\subsection{Download Docker images of
CoExp}\label{download-docker-images-of-coexp}

If everything has gone as expected, the system should now be ready to
download the two CoExp Docker images. There are two different images
because one contains the user interface (UI) of the CoExp webpage, and
the other one contains the backend of the CoExp WebPage (author
\href{https://github.com/juanbot/CoExpNets}{Juan Botia}).

In terms of the back-end of the CoExp Webpage, there are two different
docker images available:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Complete version: this docker image contains the totality of all CoExp
  networks, and it is about \textasciitilde{}4.5GB of size.
\item
  Lite version: this smaller docker image contains only the ROSMAP
  co-expression network. This image is about \textasciitilde{}1.3GB of
  size.
\end{enumerate}

Depending on which image you are interested in, the commands to execute
are:

To download the complete version:

\begin{verbatim}
$ sudo docker pull soniaruiz/coexp:r
\end{verbatim}

To download the lite version:

\begin{verbatim}
$ sudo docker pull soniaruiz/coexp:r-lite
\end{verbatim}

\subsection{Use Docker-Composer to build the
images}\label{use-docker-composer-to-build-the-images}

The next step is to make the comunication between the two docker images
possible. For that purpose, we need to download this
\href{https://github.com/SoniaRuiz/IPDGC/blob/master/complete/docker-compose.yml}{\textbf{docker-compose.yml}}
file (in case you have opted by the complete backend version), or this
\href{https://github.com/SoniaRuiz/IPDGC/blob/master/lite/docker-compose.yml}{\textbf{docker-compose.yml}}
file (if you have opted by the lite version). In any case, this file
will make possible the correct communication between the two Docker
images we downloaded in the previous step.

Additionally, the location of the downloaded \textbf{docker-compose.yml}
file is not really important, but we recommend to place it in your
\textbf{Home} folder (in case you are using a Linux machine).

Once the download has finished, use the following command to execute the
\textbf{docker-compose} file and, therefore, to run your own Docker
CoExp webpage:

\begin{verbatim}
$ sudo docker-compose up
\end{verbatim}

Finally, to test whether the execution has been correct, please type the
following URL into the address bar:

\begin{verbatim}
http://localhost:8088/
\end{verbatim}

If everything has gone as expected, you should now be able to visualize
your dockerized version of the CoExp webpage in your browser.
Congratulations!

\subsection{Juan's tutorial}\label{juans-tutorial}

Suppose we want to generate local TOM (Topology Overlap Measure) modules
from a specific network so we may, independently from the CoExpNets Web
application or within the application, plot or use networks in graph
mode. We can do it by creating their module TOMs and get the
corresponding graph in terms of a connectivity matrix we can plot.

We will exemplify this by using the frontal cortex network from GTEx V6
package as follows.

We launch all package stuff so we can start working with those networks.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(CoExpNets)}
\KeywordTok{library}\NormalTok{(CoExpGTEx)}
\NormalTok{CoExpGTEx}\OperatorTok{::}\KeywordTok{initDb}\NormalTok{()}

\NormalTok{netf =}\StringTok{ }\KeywordTok{getNetworkFromTissue}\NormalTok{(}\DataTypeTok{tissue=}\StringTok{"FCortex"}\NormalTok{,}
                            \DataTypeTok{which.on=}\StringTok{"gtexv6"}\NormalTok{,}
                            \DataTypeTok{only.file=}\NormalTok{T)}
\end{Highlighting}
\end{Shaded}

And now (we assume it is not generated yet) create the module-TOMs for
the Frontal Cortex Network as follows. As we see,

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{netf}
\end{Highlighting}
\end{Shaded}

the beta value for that network is 9, it is in the name between the
tissue and the \texttt{.it.50.rds} token in the file name.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{getModuleTOMs}\NormalTok{(}\DataTypeTok{tissue=}\StringTok{"FCortex"}\NormalTok{,}
              \DataTypeTok{beta=}\DecValTok{9}\NormalTok{,}
              \DataTypeTok{out.path=}\StringTok{"~/tmp/mytoms/"}\NormalTok{,}
              \DataTypeTok{which.one=}\StringTok{"gtexv6"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

And we can see all module-TOMs created now at the
\texttt{\textasciitilde{}/tmp/mytoms/} folder

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{list.files}\NormalTok{(}\StringTok{"~/tmp/mytoms/"}\NormalTok{,}\DataTypeTok{full.names =}\NormalTok{ F,}\DataTypeTok{recursive =}\NormalTok{ F)}
\end{Highlighting}
\end{Shaded}

And now we can get any module's connectivity matrix so we can represent
a graph for the TOM

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{getModuleTOMGraph}\NormalTok{(}\DataTypeTok{tissue=}\StringTok{"FCortex"}\NormalTok{,}
                  \DataTypeTok{which.one=}\StringTok{"gtexv6"}\NormalTok{,}
                  \DataTypeTok{module=}\StringTok{"black"}\NormalTok{,}
                  \DataTypeTok{topgenes=}\DecValTok{10}\NormalTok{,}
                  \DataTypeTok{out.path=}\StringTok{"~/tmp/mytoms/"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

And there you have it.

\chapter{GitHub}\label{github}

\section{Merge}\label{merge}

To merge two branches presenting conflicts, it is possible to indicate
which changes are preferred by using a --ours/--theirs flag:

First, we checkout to the branch that we want to apply/commit the merge:

\begin{verbatim}
> git checkout testing
> git log --oneline
\end{verbatim}

Merging would generate the following error:

\begin{verbatim}
> git merge testchanges2
warning: Cannot merge binary files: obj/Debug/netcoreapp2.1/CoExp_Web.csprojAssemblyReference.cache (HEAD vs. testchanges2)
warning: Cannot merge binary files: obj/Debug/netcoreapp2.1/CoExp_Web.Views.pdb (HEAD vs. testchanges2)
warning: Cannot merge binary files: obj/Debug/netcoreapp2.1/CoExp_Web.Views.dll (HEAD vs. testchanges2)
warning: Cannot merge binary files: .vs/CoExp_Web/v16/.suo (HEAD vs. testchanges2)
Auto-merging obj/Debug/netcoreapp2.1/CoExp_Web.csprojAssemblyReference.cache
CONFLICT (content): Merge conflict in obj/Debug/netcoreapp2.1/CoExp_Web.csprojAssemblyReference.cache
Auto-merging obj/Debug/netcoreapp2.1/CoExp_Web.Views.pdb
CONFLICT (content): Merge conflict in obj/Debug/netcoreapp2.1/CoExp_Web.Views.pdb
Auto-merging obj/Debug/netcoreapp2.1/CoExp_Web.Views.dll
CONFLICT (content): Merge conflict in obj/Debug/netcoreapp2.1/CoExp_Web.Views.dll
Auto-merging libman.json
CONFLICT (content): Merge conflict in libman.json
Auto-merging Views/Shared/_Layout.cshtml
CONFLICT (content): Merge conflict in Views/Shared/_Layout.cshtml
Auto-merging Views/Run/Help_Introduction.cshtml
Auto-merging Views/Run/Help_Catalogue.cshtml
Auto-merging Views/Run/Help_Annotation.cshtml
Auto-merging Views/Run/Help.cshtml
Auto-merging Views/Run/About.cshtml
Auto-merging .vs/CoExp_Web/v16/.suo
CONFLICT (content): Merge conflict in .vs/CoExp_Web/v16/.suo
Automatic merge failed; fix conflicts and then commit the result.
\end{verbatim}

To obtain the detail of the files that present a conflict:

\begin{verbatim}
> git log --merge
> git log --merge
commit 29cd34cc267a858fe696e86981e83bd3af1abb85 (testchanges2)
Author: Sonia Garcia <43370296+SoniaRuiz@users.noreply.github.com>
Date:   Tue Nov 24 09:03:14 2020 +0000

    commiting .suo file in testchanges2

commit 7319379b1038dc5415cf035f915d91096c70af2f (origin/testchanges2)
Author: Sonia Garcia <43370296+SoniaRuiz@users.noreply.github.com>
Date:   Tue Nov 24 00:47:04 2020 +0000

    remove unnecessary libraries

commit 5bf600b6f5f1b41b42faaca9173f33efac4958f4
Author: Sonia Garcia <43370296+SoniaRuiz@users.noreply.github.com>
Date:   Mon Nov 23 23:49:54 2020 +0000

    new built

commit 1f60b3ffb5b558849fa061efd7af1796380de30c
Author: Sonia Garcia <43370296+SoniaRuiz@users.noreply.github.com>
Date:   Mon Nov 23 23:20:51 2020 +0000

    Update .suo

    commit suo

commit d1834c7ed4a83a95e08a148c7945525e3521981c
Author: Sonia Garcia <43370296+SoniaRuiz@users.noreply.github.com>
Date:   Mon Nov 23 23:10:18 2020 +0000

> git merge --no-ff testchanges2
error: Merging is not possible because you have unmerged files.
hint: Fix them up in the work tree, and then use 'git add/rm <file>'
hint: as appropriate to mark resolution and make a commit.
fatal: Exiting because of an unresolved conflict.
\end{verbatim}

As we want to apply the comming changes, we use the flag ``--theirs'',
and the paths of the 7 conflictive files are updated:

\begin{verbatim}
> git checkout --theirs ./*
Updated 7 paths from the index
\end{verbatim}

Then, we commit the changes of those 7 files and push them to the
current repository (the `testing' repository):

\begin{verbatim}
> git branch
  master
  testchanges
  testchanges2
* testing
> git add ./*
> git commit -m "checkout --theirs testing"
[testing bcc3b5c] checkout --theirs testing
> git push
...
> git status
On branch testing
Your branch is up to date with 'origin/testing'.

nothing to commit, working tree clean
\end{verbatim}

Now, it's time to checkout to the repository we want to merge from and
apply the merge:

\begin{verbatim}
> git checkout testchanges2
Switched to branch 'testchanges2'
Your branch is ahead of 'origin/testchanges2' by 1 commit.
  (use "git push" to publish your local commits)

> git merge testing
Updating 29cd34c..bcc3b5c
Fast-forward
 Program.cs                         |  2 +-
 Properties/launchSettings.json     |  4 ++--
 Views/Run/About.cshtml             | 18 +++++++++---------
 Views/Run/Help.cshtml              |  4 ++--
 Views/Run/Help_Annotation.cshtml   |  4 ++--
 Views/Run/Help_Catalogue.cshtml    |  4 ++--
 Views/Run/Help_Introduction.cshtml |  4 ++--
 7 files changed, 20 insertions(+), 20 deletions(-)

> git checkout testing
Switched to branch 'testing'
Your branch is up to date with 'origin/testing'.

> git merge testchanges2
Already up to date.
\end{verbatim}

Once the merge is done, we can apply the commit to the `testing'
repository and push the changes. After that, both repositories
(`testchanges2' and `testing') could be considered as merged, being the
final result of their merge stored within the `testing' repository:

\begin{verbatim}
> git add ./*

> git commit -m "testing version with the plot"
[testing 7c2f7dd] testing version with the plot
 18 files changed, 90 deletions(-)
 ...

> git push origin testing
 ...
\end{verbatim}

\subsection{Troubleshooting pages of
interest}\label{troubleshooting-pages-of-interest}

\begin{itemize}
\tightlist
\item
  \href{https://stackoverflow.com/questions/13885390/in-git-merge-conflicts-how-do-i-keep-the-version-that-is-being-merged-in}{This}
\item
  \href{https://www.toolsqa.com/git/merge-branch-in-git/}{Merge a branch
  in Git}
\item
  \href{https://stackoverflow.com/questions/25576415/what-is-the-precise-meaning-of-ours-and-theirs-in-git\#:~:text=The\%20'ours'\%20in\%20Git\%20is,replayed\%20onto\%20the\%20current\%20branch}{Meaning
  of `ours' and `their' in Git}.)
\end{itemize}

\section{Stash}\label{stash}

When we are working on our GitHub project, we may want to backup some of
our changes. However, it is possible that those changes are not stable
yet, or we think they are not ready to be committed into our repository.
The stash command creates a backup copy of our changes, and keeps it
ready to be committed into the repo once we feel it is ready.

It might happen that, once we think the stash is ready to be committed,
GitHub doesn't allow us to do so because there are some `untracked'
files on it.

The only way I have found to deal with this issue, is to upload my stash
into a brand new repo, and finally merge the new repo with the one I was
intending to commit the changes to originally.

\chapter{Machine Learning Concepts}\label{machine-learning-concepts}

\section{Introduction}\label{introduction-1}

Have you ever asked yourself what is the difference between
\textbf{Artificial Intelligence} and \textbf{Machine Learning}? What
about between \textbf{supervised} and \textbf{unsupervised learning}?
Well, that's not surprising at all because trying to find out the right
answer within the huge ocean of information that is the Internet, can
become a really daunting task.

In this post, we will try first to define the most widely used Machine
Learning concepts and finally trying to give clarifying examples of each
one of them to try to help in the understanding of their meanings.

\section{Artificial Intelligence vs.~Machine
Learning}\label{artificial-intelligence-vs.machine-learning}

What is the difference between Artificial Intelligence and Machine
Learning? \textbf{Artificial Intelligence} is the concept of machines
being able to perform tasks in a way that we would consider ``smart''.
\textbf{Machine Learning} however is the current application of AI,
where we just give machines access to data and let them learn for
themselves (source
\href{https://www.forbes.com/sites/bernardmarr/2016/12/06/what-is-the-difference-between-artificial-intelligence-and-machine-learning/}{Forbes}).

The Machine Learning concept comprises different techniques whereby it
is possible to make machines learning from diverse sets of data. Among
the most important ones, we can highlight \textbf{supervised learning}
and \textbf{unsupervised learning}.

\section{Supervised learning}\label{supervised-learning}

When the training data - the data we want machines learning about -
comprises not only the input vectors but also their corresponding target
vectors.

\subsection{Classification}\label{classification}

Classification is a supervised learning method used when the target
vectors consist of a finite number of discrete categories.

The \emph{iris} dataset available in R, for instance, can be used in
classification problems because it provides different input vectors
(\emph{``Sepal.Length'', ``Sepal.Width'', ``Petal.Length'' and
``Petal.With''}) and a target vector (\emph{``Species''}) with a finite
number of categories (\emph{``setosa'', ``versicolor'' and
``virginica''}).

\begin{Shaded}
\begin{Highlighting}[]
\OperatorTok{>}\StringTok{ }\KeywordTok{summary}\NormalTok{(iris)}
\NormalTok{  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width          Species  }
\NormalTok{ Min.   }\OperatorTok{:}\FloatTok{4.300}\NormalTok{   Min.   }\OperatorTok{:}\FloatTok{2.000}\NormalTok{   Min.   }\OperatorTok{:}\FloatTok{1.000}\NormalTok{   Min.   }\OperatorTok{:}\FloatTok{0.100}\NormalTok{   setosa    }\OperatorTok{:}\DecValTok{50}  
\NormalTok{ 1st Qu.}\OperatorTok{:}\FloatTok{5.100}\NormalTok{   1st Qu.}\OperatorTok{:}\FloatTok{2.800}\NormalTok{   1st Qu.}\OperatorTok{:}\FloatTok{1.600}\NormalTok{   1st Qu.}\OperatorTok{:}\FloatTok{0.300}\NormalTok{   versicolor}\OperatorTok{:}\DecValTok{50}  
\NormalTok{ Median }\OperatorTok{:}\FloatTok{5.800}\NormalTok{   Median }\OperatorTok{:}\FloatTok{3.000}\NormalTok{   Median }\OperatorTok{:}\FloatTok{4.350}\NormalTok{   Median }\OperatorTok{:}\FloatTok{1.300}\NormalTok{   virginica }\OperatorTok{:}\DecValTok{50}  
\NormalTok{ Mean   }\OperatorTok{:}\FloatTok{5.843}\NormalTok{   Mean   }\OperatorTok{:}\FloatTok{3.057}\NormalTok{   Mean   }\OperatorTok{:}\FloatTok{3.758}\NormalTok{   Mean   }\OperatorTok{:}\FloatTok{1.199}                  
\NormalTok{ 3rd Qu.}\OperatorTok{:}\FloatTok{6.400}\NormalTok{   3rd Qu.}\OperatorTok{:}\FloatTok{3.300}\NormalTok{   3rd Qu.}\OperatorTok{:}\FloatTok{5.100}\NormalTok{   3rd Qu.}\OperatorTok{:}\FloatTok{1.800}                  
\NormalTok{ Max.   }\OperatorTok{:}\FloatTok{7.900}\NormalTok{   Max.   }\OperatorTok{:}\FloatTok{4.400}\NormalTok{   Max.   }\OperatorTok{:}\FloatTok{6.900}\NormalTok{   Max.   }\OperatorTok{:}\FloatTok{2.500}  
\end{Highlighting}
\end{Shaded}

\subsection{Regression}\label{regression}

Regression is also a supervised learning method but only used when the
target vectors consist of one or more continuous variables.

The \emph{longley} R dataset is an example of this type of data. It
presents a collection of inputs vectors (\emph{``GNP.deflator'',
``GNP'', ``Unemployed'', ``Armed.Forces'', ``Population'', ``Year''})
and a numeric vector output (\emph{``Employed''}).

\begin{Shaded}
\begin{Highlighting}[]
\OperatorTok{>}\StringTok{ }\KeywordTok{summary}\NormalTok{(longley)}
\NormalTok{  GNP.deflator         GNP          Unemployed     Armed.Forces     Population         Year         Employed    }
\NormalTok{ Min.   }\OperatorTok{:}\StringTok{ }\FloatTok{83.00}\NormalTok{   Min.   }\OperatorTok{:}\FloatTok{234.3}\NormalTok{   Min.   }\OperatorTok{:}\FloatTok{187.0}\NormalTok{   Min.   }\OperatorTok{:}\FloatTok{145.6}\NormalTok{   Min.   }\OperatorTok{:}\FloatTok{107.6}\NormalTok{   Min.   }\OperatorTok{:}\DecValTok{1947}\NormalTok{   Min.   }\OperatorTok{:}\FloatTok{60.17}  
\NormalTok{ 1st Qu.}\OperatorTok{:}\StringTok{ }\FloatTok{94.53}\NormalTok{   1st Qu.}\OperatorTok{:}\FloatTok{317.9}\NormalTok{   1st Qu.}\OperatorTok{:}\FloatTok{234.8}\NormalTok{   1st Qu.}\OperatorTok{:}\FloatTok{229.8}\NormalTok{   1st Qu.}\OperatorTok{:}\FloatTok{111.8}\NormalTok{   1st Qu.}\OperatorTok{:}\DecValTok{1951}\NormalTok{   1st Qu.}\OperatorTok{:}\FloatTok{62.71}  
\NormalTok{ Median }\OperatorTok{:}\FloatTok{100.60}\NormalTok{   Median }\OperatorTok{:}\FloatTok{381.4}\NormalTok{   Median }\OperatorTok{:}\FloatTok{314.4}\NormalTok{   Median }\OperatorTok{:}\FloatTok{271.8}\NormalTok{   Median }\OperatorTok{:}\FloatTok{116.8}\NormalTok{   Median }\OperatorTok{:}\DecValTok{1954}\NormalTok{   Median }\OperatorTok{:}\FloatTok{65.50}  
\NormalTok{ Mean   }\OperatorTok{:}\FloatTok{101.68}\NormalTok{   Mean   }\OperatorTok{:}\FloatTok{387.7}\NormalTok{   Mean   }\OperatorTok{:}\FloatTok{319.3}\NormalTok{   Mean   }\OperatorTok{:}\FloatTok{260.7}\NormalTok{   Mean   }\OperatorTok{:}\FloatTok{117.4}\NormalTok{   Mean   }\OperatorTok{:}\DecValTok{1954}\NormalTok{   Mean   }\OperatorTok{:}\FloatTok{65.32}  
\NormalTok{ 3rd Qu.}\OperatorTok{:}\FloatTok{111.25}\NormalTok{   3rd Qu.}\OperatorTok{:}\FloatTok{454.1}\NormalTok{   3rd Qu.}\OperatorTok{:}\FloatTok{384.2}\NormalTok{   3rd Qu.}\OperatorTok{:}\FloatTok{306.1}\NormalTok{   3rd Qu.}\OperatorTok{:}\FloatTok{122.3}\NormalTok{   3rd Qu.}\OperatorTok{:}\DecValTok{1958}\NormalTok{   3rd Qu.}\OperatorTok{:}\FloatTok{68.29}  
\NormalTok{ Max.   }\OperatorTok{:}\FloatTok{116.90}\NormalTok{   Max.   }\OperatorTok{:}\FloatTok{554.9}\NormalTok{   Max.   }\OperatorTok{:}\FloatTok{480.6}\NormalTok{   Max.   }\OperatorTok{:}\FloatTok{359.4}\NormalTok{   Max.   }\OperatorTok{:}\FloatTok{130.1}\NormalTok{   Max.   }\OperatorTok{:}\DecValTok{1962}\NormalTok{   Max.   }\OperatorTok{:}\FloatTok{70.55}  
\end{Highlighting}
\end{Shaded}

\section{Unsupervised learning}\label{unsupervised-learning}

When the training data consists of a set of input vectors x without any
corresponding target values. The main aim of unsupervised learning
problems can be summarised in the following ideas.

\subsection{Clustering}\label{clustering}

Grouping the input data that share different similarities into clusters
- called \textbf{clustering} (a widely known clustering algorithm is
\href{https://stanford.edu/~cpiech/cs221/handouts/kmeans.html}{K-Means}).
Below, an example dataset which can be used in a clustering problem:

\begin{Shaded}
\begin{Highlighting}[]
\OperatorTok{>}\StringTok{ }\KeywordTok{library}\NormalTok{(cluster.datasets)}
\OperatorTok{>}\StringTok{ }\KeywordTok{data}\NormalTok{(acidosis.patients)}
\OperatorTok{>}\StringTok{ }\KeywordTok{summary}\NormalTok{(acidosis.patients)}
\NormalTok{ ph.cerebrospinal.fluid    ph.blood     hco3.cerebrospinal.fluid   hco3.blood    co2.cerebrospinal.fluid   co2.blood    }
\NormalTok{ Min.   }\OperatorTok{:}\FloatTok{38.50}\NormalTok{          Min.   }\OperatorTok{:}\FloatTok{32.80}\NormalTok{   Min.   }\OperatorTok{:}\StringTok{ }\FloatTok{9.80}\NormalTok{            Min.   }\OperatorTok{:}\StringTok{ }\FloatTok{4.20}\NormalTok{   Min.   }\OperatorTok{:}\FloatTok{17.80}\NormalTok{           Min.   }\OperatorTok{:}\FloatTok{12.90}  
\NormalTok{ 1st Qu.}\OperatorTok{:}\FloatTok{45.67}\NormalTok{          1st Qu.}\OperatorTok{:}\FloatTok{37.20}\NormalTok{   1st Qu.}\OperatorTok{:}\FloatTok{19.43}\NormalTok{            1st Qu.}\OperatorTok{:}\FloatTok{19.00}\NormalTok{   1st Qu.}\OperatorTok{:}\FloatTok{40.38}\NormalTok{           1st Qu.}\OperatorTok{:}\FloatTok{30.25}  
\NormalTok{ Median }\OperatorTok{:}\FloatTok{48.05}\NormalTok{          Median }\OperatorTok{:}\FloatTok{39.55}\NormalTok{   Median }\OperatorTok{:}\FloatTok{22.65}\NormalTok{            Median }\OperatorTok{:}\FloatTok{23.30}\NormalTok{   Median }\OperatorTok{:}\FloatTok{45.25}\NormalTok{           Median }\OperatorTok{:}\FloatTok{35.70}  
\NormalTok{ Mean   }\OperatorTok{:}\FloatTok{47.52}\NormalTok{          Mean   }\OperatorTok{:}\FloatTok{41.55}\NormalTok{   Mean   }\OperatorTok{:}\FloatTok{21.91}\NormalTok{            Mean   }\OperatorTok{:}\FloatTok{22.72}\NormalTok{   Mean   }\OperatorTok{:}\FloatTok{45.57}\NormalTok{           Mean   }\OperatorTok{:}\FloatTok{36.44}  
\NormalTok{ 3rd Qu.}\OperatorTok{:}\FloatTok{49.45}\NormalTok{          3rd Qu.}\OperatorTok{:}\FloatTok{41.98}\NormalTok{   3rd Qu.}\OperatorTok{:}\FloatTok{24.52}\NormalTok{            3rd Qu.}\OperatorTok{:}\FloatTok{26.93}\NormalTok{   3rd Qu.}\OperatorTok{:}\FloatTok{52.58}\NormalTok{           3rd Qu.}\OperatorTok{:}\FloatTok{42.30}  
\NormalTok{ Max.   }\OperatorTok{:}\FloatTok{54.90}\NormalTok{          Max.   }\OperatorTok{:}\FloatTok{81.30}\NormalTok{   Max.   }\OperatorTok{:}\FloatTok{30.40}\NormalTok{            Max.   }\OperatorTok{:}\FloatTok{34.80}\NormalTok{   Max.   }\OperatorTok{:}\FloatTok{69.00}\NormalTok{           Max.   }\OperatorTok{:}\FloatTok{61.40}  
\end{Highlighting}
\end{Shaded}

\includegraphics{https://datawookie.netlify.com/img/2015/09/xclara-clusters-colour.png}
Image source:
\href{https://datawookie.netlify.com/blog/2015/10/monthofjulia-day-30-clustering/}{MonthOfJulia
Day 30: Clustering}

\subsection{Density estimation}\label{density-estimation}

Determine the distribution of data within the input space, known as
\textbf{density estimation}. In other words, the aim of density
estimation is to use statistical models to find an underlying
probability distribution that is the main reason of the observed
variables. A widely known density estimation algorithm is Kernel Density
Estimation (KDE). Although KDE algorithm has a very intimidating name,
it can be used to visualize the continuous ``shape'' of some data
instead of using its discrete version through a histogram.

\includegraphics{https://blogs.sas.com/content/iml/files/2016/07/kdecomponents2.png}
Image source:
\href{https://blogs.sas.com/content/iml/2016/07/27/visualize-kernel-density-estimate.html}{How
to visualize a kernel density estimate}

\subsection{Visualization}\label{visualization}

This unsupervised method consist of projecting the data from a
high-dimensional space down to two or three dimensions for the purpose
of \textbf{visualization}.

\bibliography{book.bib,packages.bib}

\end{document}
